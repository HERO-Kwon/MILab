{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herok\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# data path\n",
    "path_csi =  'J:\\\\Data\\\\Wi-Fi_processed\\\\'\n",
    "\n",
    "# data info\n",
    "# data path\n",
    "path_csi =  'J:\\\\Data\\\\Wi-Fi_processed\\\\'\n",
    "# data info\n",
    "df_info = pd.read_csv('data_subc_sig.csv')\n",
    "\n",
    "# parameters\n",
    "max_id = np.max(df_info['id_person'])\n",
    "max_len = int(np.max(df_info['len']))\n",
    "learning_rate = 0.1\n",
    "training_epochs = 600\n",
    "batch_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data generator\n",
    "def gen_csi(df_info,id_num,len_num):\n",
    "\n",
    "    for file in set(df_info.id.values):\n",
    "        # read sample data\n",
    "        # load and uncompress.\n",
    "        with gzip.open(path_csi+file+'.pickle.gz','rb') as f:\n",
    "            data1 = pickle.load(f)\n",
    "\n",
    "        # normalize through subc axis\n",
    "        abs_sub = np.mean(np.abs(data1),axis=(0,2,3))\n",
    "        data1_norm = data1/abs_sub[np.newaxis,:,np.newaxis,np.newaxis]\n",
    "\n",
    "        data1_abs = np.abs(data1_norm)\n",
    "        data1_ph = np.angle(data1_norm)\n",
    "        \n",
    "        # differentiation\n",
    "        data1_diff = np.diff(data1_abs,axis=0)\n",
    "        # zero pad\n",
    "        pad_len = len_num - data1_diff.shape[0]\n",
    "        data1_pad = np.pad(data1_diff,((0,pad_len),(0,0),(0,0),(0,0)),'constant',constant_values=0)\n",
    "        # reshape\n",
    "        data1_resh = data1_pad.reshape(30,max_len,6)\n",
    "        # Label\n",
    "        data1_lab = df_info[df_info.id==file]['id_person'].values[0]\n",
    "        # One hot\n",
    "        data1_one = np.eye(id_num+1)[data1_lab]\n",
    "        data1_arrays = [data1_one for one in range(30)]\n",
    "        data1_stack = np.stack(data1_arrays, axis=0)\n",
    "        \n",
    "        for i in range(len(data1_resh)):\n",
    "            yield(data1_resh[i],data1_stack[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14960, 6), (108,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator =gen_csi(df_info,max_id,max_len)\n",
    "data_s,data_l = next(generator)\n",
    "data_s.shape, data_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_idx,te_idx = train_test_split(df_info.index,test_size=0.2,random_state=10)\n",
    "#id_num = len(np.unique(df_info.id_person))\n",
    "df_train = df_info.loc[tr_idx]\n",
    "df_test = df_info.loc[te_idx]\n",
    "\n",
    "gen = lambda: (r for r in gen_csi(df_info,max_id,max_len))\n",
    "train_dataset = tf.data.Dataset().from_generator(gen, (tf.float32,tf.int32)).shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "# create general iterator\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# make datasets that we can initialize separately, but using the same structure via the common iterator\n",
    "training_init_op = iterator.make_initializer(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNmodel(in_data):\n",
    "    input1d = tf.reshape(in_data, [-1,max_len,6])\n",
    "    conv1 = tf.layers.conv1d(\n",
    "        inputs=input1d, \n",
    "        filters=100, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        #padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling1d(\n",
    "        inputs=conv1, \n",
    "        pool_size=10, \n",
    "        strides=10)\n",
    "        #padding=\"same\")\n",
    "    conv2 = tf.layers.conv1d(\n",
    "        inputs=pool1, \n",
    "        filters=250, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        #padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling1d(\n",
    "        inputs=conv2, \n",
    "        pool_size=10, \n",
    "        strides=10)\n",
    "        #padding=\"same\")\n",
    "    pool_flat = tf.layers.flatten(pool2)\n",
    "    fc = tf.layers.dense(\n",
    "        inputs= pool_flat, units=500, activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(inputs=fc, units=max_id+1)    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "logits = CNNmodel(next_element[0])\n",
    "labels = next_element[1]\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "pred = tf.argmax(logits,1)\n",
    "equal = tf.equal(pred,tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equal,tf.float32))\n",
    "\n",
    "#init\n",
    "init_op = tf.global_variables_initializer()\n",
    "training_init_op = iterator.make_initializer(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 4.681, training accuracy: 0.00%\n",
      "Epoch: 2, loss: 100224696.000, training accuracy: 5.33%\n",
      "Epoch: 3, loss: 267875.188, training accuracy: 0.00%\n",
      "Epoch: 4, loss: 347.107, training accuracy: 4.00%\n",
      "Epoch: 5, loss: 178620.078, training accuracy: 0.00%\n",
      "Epoch: 6, loss: 31587.486, training accuracy: 0.00%\n",
      "Epoch: 7, loss: 249.996, training accuracy: 1.00%\n",
      "Epoch: 8, loss: 263.833, training accuracy: 2.00%\n",
      "Epoch: 9, loss: 138.905, training accuracy: 5.33%\n",
      "Epoch: 10, loss: 67.350, training accuracy: 0.67%\n",
      "Epoch: 11, loss: 30.863, training accuracy: 0.00%\n",
      "Epoch: 12, loss: 36.220, training accuracy: 4.00%\n",
      "Epoch: 13, loss: 813.858, training accuracy: 0.00%\n",
      "Epoch: 14, loss: 19.365, training accuracy: 0.00%\n",
      "Epoch: 15, loss: 7.503, training accuracy: 1.33%\n",
      "Epoch: 16, loss: 8.147, training accuracy: 1.67%\n",
      "Epoch: 17, loss: 11.092, training accuracy: 0.00%\n",
      "Epoch: 18, loss: 9.936, training accuracy: 0.00%\n",
      "Epoch: 19, loss: 9.492, training accuracy: 0.00%\n",
      "Epoch: 20, loss: 9.138, training accuracy: 2.00%\n",
      "Epoch: 21, loss: 7.424, training accuracy: 0.67%\n",
      "Epoch: 22, loss: 6.726, training accuracy: 2.00%\n",
      "Epoch: 23, loss: 8.805, training accuracy: 2.33%\n",
      "Epoch: 24, loss: 7.034, training accuracy: 2.33%\n",
      "Epoch: 25, loss: 7.890, training accuracy: 1.33%\n",
      "Epoch: 26, loss: 8.938, training accuracy: 0.00%\n",
      "Epoch: 27, loss: 7.130, training accuracy: 0.33%\n",
      "Epoch: 28, loss: 7.695, training accuracy: 0.00%\n",
      "Epoch: 29, loss: 8.785, training accuracy: 1.67%\n",
      "Epoch: 30, loss: 7.404, training accuracy: 0.00%\n",
      "Epoch: 31, loss: 6.680, training accuracy: 3.67%\n",
      "Epoch: 32, loss: 7.720, training accuracy: 1.33%\n",
      "Epoch: 33, loss: 7.788, training accuracy: 1.00%\n",
      "Epoch: 34, loss: 13.185, training accuracy: 0.33%\n",
      "Epoch: 35, loss: 8.706, training accuracy: 0.67%\n",
      "Epoch: 36, loss: 6.151, training accuracy: 2.33%\n",
      "Epoch: 37, loss: 5.863, training accuracy: 2.00%\n",
      "Epoch: 38, loss: 6.406, training accuracy: 0.00%\n",
      "Epoch: 39, loss: 7.665, training accuracy: 0.33%\n",
      "Epoch: 40, loss: 5.171, training accuracy: 1.00%\n",
      "Epoch: 41, loss: 7.501, training accuracy: 6.33%\n",
      "Epoch: 42, loss: 5.508, training accuracy: 2.67%\n",
      "Epoch: 43, loss: 7.053, training accuracy: 1.33%\n",
      "Epoch: 44, loss: 9.587, training accuracy: 0.67%\n",
      "Epoch: 45, loss: 7.225, training accuracy: 0.33%\n",
      "Epoch: 46, loss: 6.476, training accuracy: 0.33%\n",
      "Epoch: 47, loss: 6.691, training accuracy: 0.67%\n",
      "Epoch: 48, loss: 6.531, training accuracy: 1.67%\n",
      "Epoch: 49, loss: 5.664, training accuracy: 1.67%\n",
      "Epoch: 50, loss: 4.983, training accuracy: 0.67%\n",
      "Epoch: 51, loss: 4.827, training accuracy: 0.00%\n",
      "Epoch: 52, loss: 5.719, training accuracy: 1.33%\n",
      "Epoch: 53, loss: 5.474, training accuracy: 0.00%\n",
      "Epoch: 54, loss: 5.513, training accuracy: 0.00%\n",
      "Epoch: 55, loss: 6.184, training accuracy: 0.00%\n",
      "Epoch: 56, loss: 5.269, training accuracy: 0.00%\n",
      "Epoch: 57, loss: 5.759, training accuracy: 5.00%\n",
      "Epoch: 58, loss: 5.399, training accuracy: 0.33%\n",
      "Epoch: 59, loss: 5.479, training accuracy: 0.00%\n",
      "Epoch: 60, loss: 5.786, training accuracy: 0.00%\n",
      "Epoch: 61, loss: 5.339, training accuracy: 0.00%\n",
      "Epoch: 62, loss: 5.193, training accuracy: 2.67%\n",
      "Epoch: 63, loss: 4.924, training accuracy: 0.00%\n",
      "Epoch: 64, loss: 5.098, training accuracy: 0.33%\n",
      "Epoch: 65, loss: 4.950, training accuracy: 1.33%\n",
      "Epoch: 66, loss: 5.473, training accuracy: 2.00%\n",
      "Epoch: 67, loss: 5.153, training accuracy: 0.00%\n",
      "Epoch: 68, loss: 5.525, training accuracy: 2.33%\n",
      "Epoch: 69, loss: 5.043, training accuracy: 0.00%\n",
      "Epoch: 70, loss: 5.517, training accuracy: 0.00%\n",
      "Epoch: 71, loss: 4.960, training accuracy: 0.33%\n",
      "Epoch: 72, loss: 5.423, training accuracy: 2.00%\n",
      "Epoch: 73, loss: 5.406, training accuracy: 4.33%\n",
      "Epoch: 74, loss: 5.030, training accuracy: 0.67%\n",
      "Epoch: 75, loss: 5.146, training accuracy: 0.67%\n",
      "Epoch: 76, loss: 4.903, training accuracy: 5.00%\n",
      "Epoch: 77, loss: 5.529, training accuracy: 4.33%\n",
      "Epoch: 85, loss: 4.671, training accuracy: 1.00%\n",
      "Epoch: 86, loss: 4.968, training accuracy: 0.67%\n",
      "Epoch: 87, loss: 4.863, training accuracy: 3.67%\n",
      "Epoch: 88, loss: 4.969, training accuracy: 0.33%\n",
      "Epoch: 89, loss: 4.785, training accuracy: 2.33%\n",
      "Epoch: 90, loss: 4.883, training accuracy: 1.00%\n",
      "Epoch: 91, loss: 4.788, training accuracy: 0.33%\n",
      "Epoch: 92, loss: 4.729, training accuracy: 2.00%\n",
      "Epoch: 93, loss: 4.741, training accuracy: 0.67%\n",
      "Epoch: 94, loss: 4.660, training accuracy: 2.00%\n",
      "Epoch: 95, loss: 4.743, training accuracy: 3.00%\n",
      "Epoch: 96, loss: 5.080, training accuracy: 0.00%\n",
      "Epoch: 97, loss: 4.744, training accuracy: 0.33%\n",
      "Epoch: 98, loss: 4.863, training accuracy: 0.00%\n",
      "Epoch: 99, loss: 5.074, training accuracy: 4.67%\n",
      "Epoch: 100, loss: 4.763, training accuracy: 2.00%\n",
      "Epoch: 101, loss: 4.944, training accuracy: 0.67%\n",
      "Epoch: 102, loss: 4.556, training accuracy: 0.00%\n",
      "Epoch: 103, loss: 5.489, training accuracy: 0.33%\n",
      "Epoch: 104, loss: 5.021, training accuracy: 1.33%\n",
      "Epoch: 105, loss: 4.680, training accuracy: 2.67%\n",
      "Epoch: 106, loss: 4.871, training accuracy: 1.00%\n",
      "Epoch: 107, loss: 4.828, training accuracy: 0.33%\n",
      "Epoch: 108, loss: 4.799, training accuracy: 2.00%\n",
      "Epoch: 109, loss: 4.836, training accuracy: 1.67%\n",
      "Epoch: 110, loss: 5.032, training accuracy: 1.67%\n",
      "Epoch: 111, loss: 4.699, training accuracy: 0.33%\n",
      "Epoch: 112, loss: 4.904, training accuracy: 1.00%\n",
      "Epoch: 113, loss: 4.716, training accuracy: 1.00%\n",
      "Epoch: 114, loss: 4.879, training accuracy: 1.67%\n",
      "Epoch: 115, loss: 4.689, training accuracy: 0.33%\n",
      "Epoch: 116, loss: 4.857, training accuracy: 0.00%\n",
      "Epoch: 117, loss: 4.841, training accuracy: 1.00%\n",
      "Epoch: 118, loss: 4.796, training accuracy: 0.67%\n",
      "Epoch: 119, loss: 4.842, training accuracy: 0.00%\n",
      "Epoch: 120, loss: 5.096, training accuracy: 1.33%\n",
      "Epoch: 121, loss: 4.681, training accuracy: 6.00%\n",
      "Epoch: 122, loss: 4.726, training accuracy: 1.00%\n",
      "Epoch: 123, loss: 5.374, training accuracy: 0.67%\n",
      "Epoch: 124, loss: 5.518, training accuracy: 4.00%\n",
      "Epoch: 125, loss: 5.107, training accuracy: 2.00%\n",
      "Epoch: 126, loss: 5.156, training accuracy: 2.33%\n",
      "Epoch: 127, loss: 4.587, training accuracy: 0.33%\n",
      "Epoch: 128, loss: 4.810, training accuracy: 0.00%\n",
      "Epoch: 129, loss: 4.692, training accuracy: 0.67%\n",
      "Epoch: 130, loss: 4.766, training accuracy: 0.00%\n",
      "Epoch: 131, loss: 4.648, training accuracy: 2.33%\n",
      "Epoch: 132, loss: 4.775, training accuracy: 2.67%\n",
      "Epoch: 133, loss: 5.024, training accuracy: 0.33%\n",
      "Epoch: 134, loss: 4.865, training accuracy: 0.33%\n",
      "Epoch: 135, loss: 4.815, training accuracy: 0.00%\n",
      "Epoch: 136, loss: 4.836, training accuracy: 0.67%\n",
      "Epoch: 137, loss: 4.771, training accuracy: 0.00%\n",
      "Epoch: 138, loss: 4.701, training accuracy: 1.67%\n",
      "Epoch: 139, loss: 4.622, training accuracy: 0.00%\n",
      "Epoch: 140, loss: 4.580, training accuracy: 1.67%\n",
      "Epoch: 141, loss: 4.688, training accuracy: 2.67%\n",
      "Epoch: 142, loss: 4.638, training accuracy: 1.00%\n",
      "Epoch: 143, loss: 4.677, training accuracy: 2.00%\n",
      "Epoch: 144, loss: 4.622, training accuracy: 2.67%\n",
      "Epoch: 145, loss: 4.539, training accuracy: 2.33%\n",
      "Epoch: 146, loss: 4.592, training accuracy: 2.00%\n",
      "Epoch: 147, loss: 4.475, training accuracy: 2.33%\n",
      "Epoch: 148, loss: 4.547, training accuracy: 0.00%\n",
      "Epoch: 149, loss: 4.469, training accuracy: 1.33%\n",
      "Epoch: 150, loss: 4.522, training accuracy: 2.33%\n",
      "Epoch: 151, loss: 4.536, training accuracy: 1.00%\n",
      "Epoch: 152, loss: 4.634, training accuracy: 1.67%\n",
      "Epoch: 153, loss: 4.553, training accuracy: 0.00%\n",
      "Epoch: 154, loss: 4.408, training accuracy: 6.00%\n",
      "Epoch: 155, loss: 4.479, training accuracy: 2.00%\n",
      "Epoch: 156, loss: 4.654, training accuracy: 1.00%\n",
      "Epoch: 157, loss: 4.655, training accuracy: 0.67%\n",
      "Epoch: 158, loss: 4.675, training accuracy: 2.00%\n",
      "Epoch: 159, loss: 4.756, training accuracy: 2.33%\n",
      "Epoch: 160, loss: 4.679, training accuracy: 0.33%\n",
      "Epoch: 161, loss: 4.556, training accuracy: 0.33%\n",
      "Epoch: 162, loss: 4.559, training accuracy: 2.33%\n",
      "Epoch: 163, loss: 4.680, training accuracy: 3.00%\n",
      "Epoch: 164, loss: 4.610, training accuracy: 1.33%\n",
      "Epoch: 165, loss: 4.634, training accuracy: 2.33%\n",
      "Epoch: 166, loss: 4.725, training accuracy: 0.00%\n",
      "Epoch: 167, loss: 4.614, training accuracy: 0.00%\n",
      "Epoch: 168, loss: 4.584, training accuracy: 1.67%\n",
      "Epoch: 169, loss: 4.527, training accuracy: 1.00%\n",
      "Epoch: 170, loss: 4.708, training accuracy: 1.33%\n",
      "Epoch: 171, loss: 5.371, training accuracy: 1.67%\n",
      "Epoch: 172, loss: 4.904, training accuracy: 0.33%\n",
      "Epoch: 173, loss: 5.847, training accuracy: 2.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174, loss: 4.767, training accuracy: 2.33%\n",
      "Epoch: 175, loss: 5.607, training accuracy: 1.33%\n",
      "Epoch: 176, loss: 5.463, training accuracy: 0.00%\n",
      "Epoch: 177, loss: 5.377, training accuracy: 0.00%\n",
      "Epoch: 178, loss: 4.713, training accuracy: 0.00%\n",
      "Epoch: 179, loss: 5.558, training accuracy: 4.67%\n",
      "Epoch: 180, loss: 5.246, training accuracy: 2.00%\n",
      "Epoch: 181, loss: 4.660, training accuracy: 4.00%\n",
      "Epoch: 182, loss: 5.645, training accuracy: 1.00%\n",
      "Epoch: 183, loss: 5.157, training accuracy: 2.33%\n",
      "Epoch: 184, loss: 5.477, training accuracy: 2.67%\n",
      "Epoch: 185, loss: 5.228, training accuracy: 1.67%\n",
      "Epoch: 186, loss: 5.382, training accuracy: 0.00%\n",
      "Epoch: 187, loss: 4.762, training accuracy: 1.33%\n",
      "Epoch: 188, loss: 4.764, training accuracy: 1.67%\n",
      "Epoch: 189, loss: 5.112, training accuracy: 0.33%\n",
      "Epoch: 190, loss: 4.686, training accuracy: 0.00%\n",
      "Epoch: 191, loss: 5.054, training accuracy: 0.67%\n",
      "Epoch: 192, loss: 4.717, training accuracy: 0.67%\n",
      "Epoch: 193, loss: 4.990, training accuracy: 2.00%\n",
      "Epoch: 194, loss: 4.708, training accuracy: 0.67%\n",
      "Epoch: 195, loss: 4.740, training accuracy: 1.33%\n",
      "Epoch: 196, loss: 4.584, training accuracy: 2.67%\n",
      "Epoch: 197, loss: 4.719, training accuracy: 2.67%\n",
      "Epoch: 198, loss: 4.949, training accuracy: 6.67%\n",
      "Epoch: 199, loss: 4.534, training accuracy: 2.67%\n",
      "Epoch: 200, loss: 5.253, training accuracy: 0.00%\n",
      "Epoch: 201, loss: 5.083, training accuracy: 2.00%\n",
      "Epoch: 202, loss: 4.967, training accuracy: 0.33%\n",
      "Epoch: 203, loss: 5.047, training accuracy: 0.33%\n",
      "Epoch: 204, loss: 4.655, training accuracy: 0.33%\n",
      "Epoch: 205, loss: 4.932, training accuracy: 0.67%\n",
      "Epoch: 206, loss: 4.807, training accuracy: 2.00%\n",
      "Epoch: 207, loss: 5.030, training accuracy: 2.00%\n",
      "Epoch: 208, loss: 5.083, training accuracy: 4.00%\n",
      "Epoch: 209, loss: 4.963, training accuracy: 5.00%\n",
      "Epoch: 210, loss: 4.866, training accuracy: 2.67%\n",
      "Epoch: 211, loss: 4.765, training accuracy: 0.67%\n",
      "Epoch: 212, loss: 5.016, training accuracy: 0.33%\n",
      "Epoch: 213, loss: 4.788, training accuracy: 5.33%\n",
      "Epoch: 214, loss: 5.154, training accuracy: 1.00%\n",
      "Epoch: 215, loss: 4.702, training accuracy: 1.67%\n",
      "Epoch: 216, loss: 5.690, training accuracy: 0.67%\n",
      "Epoch: 217, loss: 5.615, training accuracy: 0.33%\n",
      "Epoch: 218, loss: 5.472, training accuracy: 0.33%\n",
      "Epoch: 219, loss: 6.264, training accuracy: 0.00%\n",
      "Epoch: 220, loss: 4.956, training accuracy: 0.33%\n",
      "Epoch: 221, loss: 5.090, training accuracy: 3.33%\n",
      "Epoch: 222, loss: 5.575, training accuracy: 0.00%\n",
      "Epoch: 223, loss: 4.580, training accuracy: 2.33%\n",
      "Epoch: 224, loss: 5.261, training accuracy: 0.33%\n",
      "Epoch: 225, loss: 5.813, training accuracy: 1.33%\n",
      "Epoch: 226, loss: 5.239, training accuracy: 1.00%\n",
      "Epoch: 227, loss: 5.345, training accuracy: 0.33%\n",
      "Epoch: 228, loss: 4.962, training accuracy: 5.67%\n",
      "Epoch: 229, loss: 5.395, training accuracy: 3.00%\n",
      "Epoch: 230, loss: 4.948, training accuracy: 1.00%\n",
      "Epoch: 231, loss: 5.055, training accuracy: 0.00%\n",
      "Epoch: 232, loss: 4.933, training accuracy: 0.00%\n",
      "Epoch: 233, loss: 4.995, training accuracy: 0.67%\n",
      "Epoch: 234, loss: 5.138, training accuracy: 1.67%\n",
      "Epoch: 235, loss: 4.879, training accuracy: 1.33%\n",
      "Epoch: 236, loss: 4.689, training accuracy: 1.00%\n",
      "Epoch: 237, loss: 4.805, training accuracy: 0.67%\n",
      "Epoch: 238, loss: 4.616, training accuracy: 0.67%\n",
      "Epoch: 239, loss: 4.884, training accuracy: 3.00%\n",
      "Epoch: 240, loss: 4.742, training accuracy: 0.33%\n",
      "Epoch: 241, loss: 4.819, training accuracy: 1.33%\n",
      "Epoch: 242, loss: 4.676, training accuracy: 0.67%\n",
      "Epoch: 243, loss: 4.863, training accuracy: 2.00%\n",
      "Epoch: 244, loss: 4.793, training accuracy: 1.67%\n",
      "Epoch: 245, loss: 5.053, training accuracy: 1.00%\n",
      "Epoch: 246, loss: 4.908, training accuracy: 0.00%\n",
      "Epoch: 247, loss: 4.831, training accuracy: 1.67%\n",
      "Epoch: 248, loss: 4.674, training accuracy: 3.00%\n",
      "Epoch: 249, loss: 4.778, training accuracy: 1.00%\n",
      "Epoch: 250, loss: 4.665, training accuracy: 0.67%\n",
      "Epoch: 251, loss: 4.540, training accuracy: 2.33%\n",
      "Epoch: 252, loss: 4.696, training accuracy: 3.33%\n",
      "Epoch: 253, loss: 4.589, training accuracy: 3.67%\n",
      "Epoch: 254, loss: 4.779, training accuracy: 2.00%\n",
      "Epoch: 255, loss: 4.557, training accuracy: 2.67%\n",
      "Epoch: 256, loss: 4.648, training accuracy: 1.33%\n",
      "Epoch: 257, loss: 4.810, training accuracy: 1.33%\n",
      "Epoch: 258, loss: 4.901, training accuracy: 1.67%\n",
      "Epoch: 259, loss: 5.452, training accuracy: 3.33%\n",
      "Epoch: 260, loss: 4.642, training accuracy: 1.67%\n",
      "Epoch: 261, loss: 4.949, training accuracy: 0.00%\n",
      "Epoch: 262, loss: 4.474, training accuracy: 3.33%\n",
      "Epoch: 263, loss: 5.381, training accuracy: 0.00%\n",
      "Epoch: 264, loss: 4.847, training accuracy: 0.67%\n",
      "Epoch: 265, loss: 5.120, training accuracy: 0.00%\n",
      "Epoch: 266, loss: 4.756, training accuracy: 0.00%\n",
      "Epoch: 267, loss: 5.650, training accuracy: 1.33%\n",
      "Epoch: 268, loss: 4.762, training accuracy: 0.00%\n",
      "Epoch: 269, loss: 5.019, training accuracy: 0.00%\n",
      "Epoch: 270, loss: 4.687, training accuracy: 1.33%\n",
      "Epoch: 271, loss: 5.069, training accuracy: 2.00%\n",
      "Epoch: 272, loss: 4.696, training accuracy: 3.33%\n",
      "Epoch: 273, loss: 4.706, training accuracy: 6.00%\n",
      "Epoch: 274, loss: 4.760, training accuracy: 4.33%\n",
      "Epoch: 275, loss: 4.821, training accuracy: 1.67%\n",
      "Epoch: 276, loss: 4.553, training accuracy: 1.00%\n",
      "Epoch: 277, loss: 4.728, training accuracy: 1.33%\n",
      "Epoch: 278, loss: 4.679, training accuracy: 2.00%\n",
      "Epoch: 279, loss: 4.732, training accuracy: 4.00%\n",
      "Epoch: 280, loss: 4.892, training accuracy: 0.00%\n",
      "Epoch: 281, loss: 4.576, training accuracy: 4.00%\n",
      "Epoch: 282, loss: 5.965, training accuracy: 3.00%\n",
      "Epoch: 283, loss: 5.396, training accuracy: 0.00%\n",
      "Epoch: 284, loss: 6.591, training accuracy: 0.00%\n",
      "Epoch: 285, loss: 7.219, training accuracy: 0.00%\n",
      "Epoch: 286, loss: 6.822, training accuracy: 2.00%\n",
      "Epoch: 287, loss: 7.254, training accuracy: 1.00%\n",
      "Epoch: 288, loss: 6.510, training accuracy: 1.00%\n",
      "Epoch: 289, loss: 6.241, training accuracy: 1.00%\n",
      "Epoch: 290, loss: 5.572, training accuracy: 3.00%\n",
      "Epoch: 291, loss: 5.896, training accuracy: 4.00%\n",
      "Epoch: 292, loss: 5.420, training accuracy: 3.33%\n",
      "Epoch: 293, loss: 5.472, training accuracy: 1.33%\n",
      "Epoch: 294, loss: 5.057, training accuracy: 1.00%\n",
      "Epoch: 295, loss: 6.169, training accuracy: 2.00%\n",
      "Epoch: 296, loss: 6.036, training accuracy: 0.33%\n",
      "Epoch: 297, loss: 6.411, training accuracy: 0.00%\n",
      "Epoch: 298, loss: 5.615, training accuracy: 0.00%\n",
      "Epoch: 299, loss: 6.132, training accuracy: 2.67%\n",
      "Epoch: 300, loss: 4.848, training accuracy: 0.67%\n",
      "Epoch: 301, loss: 5.352, training accuracy: 2.33%\n",
      "Epoch: 302, loss: 5.189, training accuracy: 0.33%\n",
      "Epoch: 303, loss: 4.735, training accuracy: 1.33%\n",
      "Epoch: 304, loss: 4.913, training accuracy: 2.00%\n",
      "Epoch: 305, loss: 4.953, training accuracy: 2.67%\n",
      "Epoch: 306, loss: 5.371, training accuracy: 2.33%\n",
      "Epoch: 307, loss: 5.089, training accuracy: 0.67%\n",
      "Epoch: 308, loss: 5.306, training accuracy: 0.67%\n",
      "Epoch: 309, loss: 4.752, training accuracy: 1.00%\n",
      "Epoch: 310, loss: 5.340, training accuracy: 1.33%\n",
      "Epoch: 311, loss: 4.747, training accuracy: 1.00%\n",
      "Epoch: 312, loss: 6.006, training accuracy: 0.00%\n",
      "Epoch: 313, loss: 4.963, training accuracy: 0.67%\n",
      "Epoch: 314, loss: 5.075, training accuracy: 1.00%\n",
      "Epoch: 315, loss: 4.951, training accuracy: 1.67%\n",
      "Epoch: 316, loss: 5.630, training accuracy: 0.00%\n",
      "Epoch: 317, loss: 4.907, training accuracy: 0.33%\n",
      "Epoch: 318, loss: 5.370, training accuracy: 0.00%\n",
      "Epoch: 319, loss: 4.711, training accuracy: 0.33%\n",
      "Epoch: 320, loss: 4.662, training accuracy: 5.00%\n",
      "Epoch: 321, loss: 5.000, training accuracy: 1.00%\n",
      "Epoch: 322, loss: 4.583, training accuracy: 0.67%\n",
      "Epoch: 323, loss: 4.716, training accuracy: 0.00%\n",
      "Epoch: 324, loss: 4.861, training accuracy: 3.33%\n",
      "Epoch: 325, loss: 4.762, training accuracy: 2.00%\n",
      "Epoch: 326, loss: 4.976, training accuracy: 0.33%\n",
      "Epoch: 327, loss: 4.740, training accuracy: 0.67%\n",
      "Epoch: 328, loss: 4.808, training accuracy: 0.33%\n",
      "Epoch: 329, loss: 4.962, training accuracy: 0.00%\n",
      "Epoch: 330, loss: 4.776, training accuracy: 1.00%\n",
      "Epoch: 331, loss: 4.835, training accuracy: 1.67%\n",
      "Epoch: 332, loss: 4.796, training accuracy: 0.67%\n",
      "Epoch: 333, loss: 4.893, training accuracy: 6.67%\n",
      "Epoch: 334, loss: 4.642, training accuracy: 0.33%\n",
      "Epoch: 335, loss: 4.736, training accuracy: 0.00%\n",
      "Epoch: 336, loss: 4.822, training accuracy: 0.33%\n",
      "Epoch: 337, loss: 4.610, training accuracy: 3.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 338, loss: 4.541, training accuracy: 1.33%\n",
      "Epoch: 339, loss: 4.524, training accuracy: 3.33%\n",
      "Epoch: 340, loss: 4.856, training accuracy: 0.33%\n",
      "Epoch: 341, loss: 4.649, training accuracy: 2.67%\n",
      "Epoch: 342, loss: 4.705, training accuracy: 2.33%\n",
      "Epoch: 343, loss: 4.850, training accuracy: 1.00%\n",
      "Epoch: 344, loss: 4.604, training accuracy: 1.00%\n",
      "Epoch: 345, loss: 4.814, training accuracy: 0.67%\n",
      "Epoch: 346, loss: 4.774, training accuracy: 0.00%\n",
      "Epoch: 347, loss: 4.856, training accuracy: 0.33%\n",
      "Epoch: 348, loss: 4.705, training accuracy: 0.67%\n",
      "Epoch: 349, loss: 4.588, training accuracy: 1.33%\n",
      "Epoch: 350, loss: 4.604, training accuracy: 1.33%\n",
      "Epoch: 351, loss: 4.496, training accuracy: 0.33%\n",
      "Epoch: 352, loss: 4.496, training accuracy: 2.00%\n",
      "Epoch: 353, loss: 4.456, training accuracy: 1.00%\n",
      "Epoch: 354, loss: 4.690, training accuracy: 0.67%\n",
      "Epoch: 355, loss: 4.584, training accuracy: 0.00%\n",
      "Epoch: 356, loss: 4.628, training accuracy: 2.67%\n",
      "Epoch: 357, loss: 4.611, training accuracy: 0.00%\n",
      "Epoch: 358, loss: 4.635, training accuracy: 0.00%\n",
      "Epoch: 359, loss: 4.745, training accuracy: 1.67%\n",
      "Epoch: 360, loss: 4.609, training accuracy: 1.00%\n",
      "Epoch: 361, loss: 4.598, training accuracy: 0.00%\n",
      "Epoch: 362, loss: 4.765, training accuracy: 1.00%\n",
      "Epoch: 363, loss: 4.740, training accuracy: 0.00%\n",
      "Epoch: 364, loss: 4.805, training accuracy: 2.67%\n",
      "Epoch: 365, loss: 4.691, training accuracy: 4.33%\n",
      "Epoch: 366, loss: 4.762, training accuracy: 2.33%\n",
      "Epoch: 367, loss: 4.947, training accuracy: 0.00%\n",
      "Epoch: 368, loss: 4.881, training accuracy: 0.00%\n",
      "Epoch: 369, loss: 4.794, training accuracy: 0.00%\n",
      "Epoch: 370, loss: 4.206, training accuracy: 2.67%\n",
      "Epoch: 371, loss: 5.164, training accuracy: 4.33%\n",
      "Epoch: 372, loss: 6.099, training accuracy: 0.67%\n",
      "Epoch: 373, loss: 6.669, training accuracy: 0.67%\n",
      "Epoch: 374, loss: 6.056, training accuracy: 0.00%\n",
      "Epoch: 375, loss: 4.779, training accuracy: 0.33%\n",
      "Epoch: 376, loss: 5.458, training accuracy: 2.67%\n",
      "Epoch: 377, loss: 5.183, training accuracy: 3.67%\n",
      "Epoch: 378, loss: 5.685, training accuracy: 3.33%\n",
      "Epoch: 379, loss: 5.964, training accuracy: 0.33%\n",
      "Epoch: 380, loss: 5.745, training accuracy: 0.33%\n",
      "Epoch: 381, loss: 5.472, training accuracy: 4.00%\n",
      "Epoch: 382, loss: 5.295, training accuracy: 4.00%\n",
      "Epoch: 383, loss: 5.098, training accuracy: 1.33%\n",
      "Epoch: 384, loss: 4.761, training accuracy: 0.33%\n",
      "Epoch: 385, loss: 5.628, training accuracy: 1.33%\n",
      "Epoch: 386, loss: 5.279, training accuracy: 0.67%\n",
      "Epoch: 387, loss: 5.240, training accuracy: 0.00%\n",
      "Epoch: 388, loss: 4.690, training accuracy: 0.33%\n",
      "Epoch: 389, loss: 4.927, training accuracy: 0.00%\n",
      "Epoch: 390, loss: 5.551, training accuracy: 0.67%\n",
      "Epoch: 391, loss: 5.381, training accuracy: 4.33%\n",
      "Epoch: 392, loss: 5.200, training accuracy: 0.00%\n",
      "Epoch: 393, loss: 5.026, training accuracy: 0.00%\n",
      "Epoch: 394, loss: 4.939, training accuracy: 0.00%\n",
      "Epoch: 395, loss: 5.201, training accuracy: 0.67%\n",
      "Epoch: 396, loss: 4.704, training accuracy: 3.67%\n",
      "Epoch: 397, loss: 4.948, training accuracy: 0.00%\n",
      "Epoch: 398, loss: 4.924, training accuracy: 4.67%\n",
      "Epoch: 399, loss: 4.825, training accuracy: 6.00%\n",
      "Epoch: 400, loss: 4.733, training accuracy: 1.00%\n",
      "Epoch: 401, loss: 4.727, training accuracy: 1.00%\n",
      "Epoch: 402, loss: 4.649, training accuracy: 0.33%\n",
      "Epoch: 403, loss: 4.670, training accuracy: 3.67%\n",
      "Epoch: 404, loss: 4.682, training accuracy: 1.00%\n",
      "Epoch: 405, loss: 4.688, training accuracy: 0.67%\n",
      "Epoch: 406, loss: 4.656, training accuracy: 2.67%\n",
      "Epoch: 407, loss: 4.808, training accuracy: 1.33%\n",
      "Epoch: 408, loss: 4.623, training accuracy: 2.00%\n",
      "Epoch: 409, loss: 4.797, training accuracy: 0.00%\n",
      "Epoch: 410, loss: 4.854, training accuracy: 0.33%\n",
      "Epoch: 411, loss: 4.707, training accuracy: 0.33%\n",
      "Epoch: 412, loss: 4.825, training accuracy: 0.00%\n",
      "Epoch: 413, loss: 4.728, training accuracy: 3.33%\n",
      "Epoch: 414, loss: 4.566, training accuracy: 0.67%\n",
      "Epoch: 415, loss: 4.820, training accuracy: 3.00%\n",
      "Epoch: 416, loss: 4.921, training accuracy: 1.67%\n",
      "Epoch: 417, loss: 5.077, training accuracy: 0.00%\n",
      "Epoch: 418, loss: 4.701, training accuracy: 1.00%\n",
      "Epoch: 419, loss: 4.878, training accuracy: 1.67%\n",
      "Epoch: 420, loss: 4.728, training accuracy: 1.00%\n",
      "Epoch: 421, loss: 4.876, training accuracy: 1.00%\n",
      "Epoch: 422, loss: 4.644, training accuracy: 0.67%\n",
      "Epoch: 423, loss: 5.037, training accuracy: 2.67%\n",
      "Epoch: 424, loss: 4.653, training accuracy: 1.00%\n",
      "Epoch: 425, loss: 5.246, training accuracy: 1.00%\n",
      "Epoch: 426, loss: 4.792, training accuracy: 1.33%\n",
      "Epoch: 427, loss: 5.032, training accuracy: 3.00%\n",
      "Epoch: 428, loss: 4.569, training accuracy: 4.33%\n",
      "Epoch: 429, loss: 4.524, training accuracy: 2.33%\n",
      "Epoch: 430, loss: 5.108, training accuracy: 2.00%\n",
      "Epoch: 431, loss: 4.699, training accuracy: 0.33%\n",
      "Epoch: 432, loss: 4.917, training accuracy: 2.33%\n",
      "Epoch: 433, loss: 4.772, training accuracy: 0.00%\n",
      "Epoch: 434, loss: 4.822, training accuracy: 5.00%\n",
      "Epoch: 435, loss: 4.833, training accuracy: 3.33%\n",
      "Epoch: 436, loss: 4.744, training accuracy: 4.00%\n",
      "Epoch: 437, loss: 4.815, training accuracy: 2.33%\n",
      "Epoch: 438, loss: 5.011, training accuracy: 1.00%\n",
      "Epoch: 439, loss: 4.716, training accuracy: 0.67%\n",
      "Epoch: 440, loss: 4.929, training accuracy: 2.67%\n",
      "Epoch: 441, loss: 4.967, training accuracy: 0.00%\n",
      "Epoch: 442, loss: 5.019, training accuracy: 1.00%\n",
      "Epoch: 443, loss: 5.216, training accuracy: 2.00%\n",
      "Epoch: 444, loss: 4.927, training accuracy: 4.33%\n",
      "Epoch: 445, loss: 4.620, training accuracy: 3.33%\n",
      "Epoch: 446, loss: 4.520, training accuracy: 8.67%\n",
      "Epoch: 447, loss: 4.704, training accuracy: 4.00%\n",
      "Epoch: 448, loss: 4.760, training accuracy: 1.33%\n",
      "Epoch: 449, loss: 4.906, training accuracy: 0.33%\n",
      "Epoch: 450, loss: 4.657, training accuracy: 5.00%\n",
      "Epoch: 451, loss: 4.614, training accuracy: 0.33%\n",
      "Epoch: 452, loss: 4.780, training accuracy: 1.67%\n",
      "Epoch: 453, loss: 4.780, training accuracy: 1.67%\n",
      "Epoch: 454, loss: 5.171, training accuracy: 1.00%\n",
      "Epoch: 455, loss: 4.949, training accuracy: 0.00%\n",
      "Epoch: 456, loss: 4.844, training accuracy: 0.00%\n",
      "Epoch: 457, loss: 5.011, training accuracy: 2.67%\n",
      "Epoch: 458, loss: 5.186, training accuracy: 1.67%\n",
      "Epoch: 459, loss: 4.685, training accuracy: 0.00%\n",
      "Epoch: 460, loss: 4.617, training accuracy: 2.67%\n",
      "Epoch: 461, loss: 4.579, training accuracy: 0.00%\n",
      "Epoch: 462, loss: 5.034, training accuracy: 0.00%\n",
      "Epoch: 463, loss: 4.997, training accuracy: 2.67%\n",
      "Epoch: 464, loss: 4.657, training accuracy: 0.33%\n",
      "Epoch: 465, loss: 4.744, training accuracy: 2.00%\n",
      "Epoch: 466, loss: 4.628, training accuracy: 3.33%\n",
      "Epoch: 467, loss: 5.144, training accuracy: 4.00%\n",
      "Epoch: 468, loss: 4.869, training accuracy: 3.00%\n",
      "Epoch: 469, loss: 4.898, training accuracy: 0.00%\n",
      "Epoch: 470, loss: 4.707, training accuracy: 0.00%\n",
      "Epoch: 471, loss: 5.211, training accuracy: 0.00%\n",
      "Epoch: 472, loss: 4.688, training accuracy: 1.33%\n",
      "Epoch: 473, loss: 5.142, training accuracy: 2.00%\n",
      "Epoch: 474, loss: 4.828, training accuracy: 2.67%\n",
      "Epoch: 475, loss: 4.827, training accuracy: 8.00%\n",
      "Epoch: 476, loss: 5.115, training accuracy: 0.33%\n",
      "Epoch: 477, loss: 5.062, training accuracy: 0.00%\n",
      "Epoch: 478, loss: 4.944, training accuracy: 2.67%\n",
      "Epoch: 479, loss: 4.807, training accuracy: 2.33%\n",
      "Epoch: 480, loss: 4.705, training accuracy: 0.00%\n",
      "Epoch: 481, loss: 4.651, training accuracy: 1.67%\n",
      "Epoch: 482, loss: 4.761, training accuracy: 0.67%\n",
      "Epoch: 483, loss: 4.868, training accuracy: 0.00%\n",
      "Epoch: 484, loss: 4.682, training accuracy: 1.00%\n",
      "Epoch: 485, loss: 4.889, training accuracy: 0.67%\n",
      "Epoch: 486, loss: 4.729, training accuracy: 0.33%\n",
      "Epoch: 487, loss: 4.880, training accuracy: 0.33%\n",
      "Epoch: 488, loss: 5.140, training accuracy: 1.67%\n",
      "Epoch: 489, loss: 5.052, training accuracy: 0.67%\n",
      "Epoch: 490, loss: 4.837, training accuracy: 0.67%\n",
      "Epoch: 491, loss: 4.636, training accuracy: 2.00%\n",
      "Epoch: 492, loss: 4.731, training accuracy: 4.33%\n",
      "Epoch: 493, loss: 4.609, training accuracy: 2.00%\n",
      "Epoch: 494, loss: 5.033, training accuracy: 1.33%\n",
      "Epoch: 495, loss: 4.680, training accuracy: 0.33%\n",
      "Epoch: 496, loss: 4.777, training accuracy: 3.00%\n",
      "Epoch: 497, loss: 4.524, training accuracy: 2.67%\n",
      "Epoch: 498, loss: 4.652, training accuracy: 2.00%\n",
      "Epoch: 499, loss: 5.078, training accuracy: 1.67%\n",
      "Epoch: 500, loss: 4.940, training accuracy: 2.67%\n",
      "Epoch: 501, loss: 5.017, training accuracy: 0.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 502, loss: 4.893, training accuracy: 0.00%\n",
      "Epoch: 503, loss: 4.794, training accuracy: 1.00%\n",
      "Epoch: 504, loss: 4.690, training accuracy: 5.00%\n",
      "Epoch: 505, loss: 4.875, training accuracy: 0.67%\n",
      "Epoch: 506, loss: 4.588, training accuracy: 0.00%\n",
      "Epoch: 507, loss: 4.984, training accuracy: 0.00%\n",
      "Epoch: 508, loss: 4.751, training accuracy: 6.00%\n",
      "Epoch: 509, loss: 5.065, training accuracy: 2.00%\n",
      "Epoch: 510, loss: 4.969, training accuracy: 1.00%\n",
      "Epoch: 511, loss: 4.900, training accuracy: 1.00%\n",
      "Epoch: 512, loss: 4.712, training accuracy: 4.67%\n",
      "Epoch: 513, loss: 5.046, training accuracy: 0.33%\n",
      "Epoch: 514, loss: 5.204, training accuracy: 0.00%\n",
      "Epoch: 515, loss: 5.080, training accuracy: 0.33%\n",
      "Epoch: 516, loss: 5.207, training accuracy: 4.00%\n",
      "Epoch: 517, loss: 4.861, training accuracy: 2.00%\n",
      "Epoch: 518, loss: 5.230, training accuracy: 0.00%\n",
      "Epoch: 519, loss: 4.577, training accuracy: 5.33%\n",
      "Epoch: 520, loss: 4.742, training accuracy: 6.33%\n",
      "Epoch: 521, loss: 4.743, training accuracy: 4.00%\n",
      "Epoch: 522, loss: 4.491, training accuracy: 2.33%\n",
      "Epoch: 523, loss: 4.707, training accuracy: 6.33%\n",
      "Epoch: 524, loss: 4.597, training accuracy: 3.33%\n",
      "Epoch: 525, loss: 4.755, training accuracy: 2.00%\n",
      "Epoch: 526, loss: 4.824, training accuracy: 4.00%\n",
      "Epoch: 527, loss: 4.661, training accuracy: 1.00%\n",
      "Epoch: 528, loss: 4.666, training accuracy: 1.67%\n",
      "Epoch: 529, loss: 4.653, training accuracy: 0.67%\n",
      "Epoch: 530, loss: 4.822, training accuracy: 0.33%\n",
      "Epoch: 531, loss: 5.239, training accuracy: 4.00%\n",
      "Epoch: 532, loss: 4.869, training accuracy: 2.33%\n",
      "Epoch: 533, loss: 5.131, training accuracy: 0.00%\n",
      "Epoch: 534, loss: 5.780, training accuracy: 0.00%\n",
      "Epoch: 535, loss: 6.904, training accuracy: 0.00%\n",
      "Epoch: 536, loss: 6.423, training accuracy: 2.00%\n",
      "Epoch: 537, loss: 5.817, training accuracy: 0.67%\n",
      "Epoch: 538, loss: 6.583, training accuracy: 2.33%\n",
      "Epoch: 539, loss: 6.178, training accuracy: 3.33%\n",
      "Epoch: 540, loss: 6.520, training accuracy: 1.67%\n",
      "Epoch: 541, loss: 7.973, training accuracy: 1.33%\n",
      "Epoch: 542, loss: 5.161, training accuracy: 0.67%\n",
      "Epoch: 543, loss: 6.149, training accuracy: 2.00%\n",
      "Epoch: 544, loss: 10.703, training accuracy: 0.33%\n",
      "Epoch: 545, loss: 12.171, training accuracy: 0.33%\n",
      "Epoch: 546, loss: 11.890, training accuracy: 0.33%\n",
      "Epoch: 547, loss: 11.140, training accuracy: 4.67%\n",
      "Epoch: 548, loss: 8.372, training accuracy: 2.33%\n",
      "Epoch: 549, loss: 9.013, training accuracy: 0.33%\n",
      "Epoch: 550, loss: 7.667, training accuracy: 2.67%\n",
      "Epoch: 551, loss: 7.245, training accuracy: 2.00%\n",
      "Epoch: 552, loss: 6.017, training accuracy: 2.00%\n",
      "Epoch: 553, loss: 7.044, training accuracy: 0.33%\n",
      "Epoch: 554, loss: 6.638, training accuracy: 0.33%\n",
      "Epoch: 555, loss: 7.943, training accuracy: 0.00%\n",
      "Epoch: 556, loss: 6.988, training accuracy: 2.67%\n",
      "Epoch: 557, loss: 6.504, training accuracy: 0.67%\n",
      "Epoch: 558, loss: 5.744, training accuracy: 1.67%\n",
      "Epoch: 559, loss: 5.459, training accuracy: 0.67%\n",
      "Epoch: 560, loss: 5.595, training accuracy: 1.67%\n",
      "Epoch: 561, loss: 5.540, training accuracy: 4.67%\n",
      "Epoch: 562, loss: 5.733, training accuracy: 0.67%\n",
      "Epoch: 563, loss: 5.877, training accuracy: 1.00%\n",
      "Epoch: 564, loss: 5.268, training accuracy: 3.67%\n",
      "Epoch: 565, loss: 5.472, training accuracy: 0.00%\n",
      "Epoch: 566, loss: 5.942, training accuracy: 1.67%\n",
      "Epoch: 567, loss: 5.235, training accuracy: 2.00%\n",
      "Epoch: 568, loss: 4.967, training accuracy: 1.00%\n",
      "Epoch: 569, loss: 5.283, training accuracy: 0.33%\n",
      "Epoch: 570, loss: 4.986, training accuracy: 3.00%\n",
      "Epoch: 571, loss: 5.275, training accuracy: 0.33%\n",
      "Epoch: 572, loss: 4.906, training accuracy: 2.67%\n",
      "Epoch: 573, loss: 5.045, training accuracy: 1.67%\n",
      "Epoch: 574, loss: 5.060, training accuracy: 1.33%\n",
      "Epoch: 575, loss: 4.812, training accuracy: 1.33%\n",
      "Epoch: 576, loss: 4.760, training accuracy: 4.00%\n",
      "Epoch: 577, loss: 5.165, training accuracy: 2.67%\n",
      "Epoch: 578, loss: 4.962, training accuracy: 2.00%\n",
      "Epoch: 579, loss: 4.848, training accuracy: 2.00%\n",
      "Epoch: 580, loss: 4.611, training accuracy: 1.33%\n",
      "Epoch: 581, loss: 4.703, training accuracy: 0.00%\n",
      "Epoch: 582, loss: 4.734, training accuracy: 0.67%\n",
      "Epoch: 583, loss: 4.756, training accuracy: 5.67%\n",
      "Epoch: 584, loss: 4.833, training accuracy: 3.33%\n",
      "Epoch: 585, loss: 4.617, training accuracy: 0.33%\n",
      "Epoch: 586, loss: 4.941, training accuracy: 2.33%\n",
      "Epoch: 587, loss: 4.653, training accuracy: 2.33%\n",
      "Epoch: 588, loss: 4.849, training accuracy: 1.00%\n",
      "Epoch: 589, loss: 4.918, training accuracy: 4.00%\n",
      "Epoch: 590, loss: 4.842, training accuracy: 2.00%\n",
      "Epoch: 591, loss: 4.770, training accuracy: 1.00%\n",
      "Epoch: 592, loss: 4.537, training accuracy: 0.00%\n",
      "Epoch: 593, loss: 4.638, training accuracy: 1.67%\n",
      "Epoch: 594, loss: 4.673, training accuracy: 3.00%\n",
      "Epoch: 595, loss: 4.695, training accuracy: 1.00%\n",
      "Epoch: 596, loss: 4.544, training accuracy: 0.67%\n",
      "Epoch: 597, loss: 5.126, training accuracy: 0.33%\n",
      "Epoch: 598, loss: 4.901, training accuracy: 2.00%\n",
      "Epoch: 599, loss: 4.995, training accuracy: 0.00%\n",
      "Epoch: 600, loss: 4.700, training accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# run the training\n",
    "epochs = training_epochs\n",
    "results = pd.DataFrame(columns=['loss','accuracy'])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(training_init_op)\n",
    "    for i in range(epochs):\n",
    "        log,lab,pr = sess.run([logits,labels,pred])\n",
    "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "        \n",
    "        print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i+1, l, acc * 100))\n",
    "        '''\n",
    "        # now setup the validation run\n",
    "        test_iters = 1\n",
    "        # re-initialize the iterator, but this time with validation data\n",
    "        sess.run(test_init_op)\n",
    "        test_l, test_acc = sess.run([loss, accuracy])\n",
    "        print(\"Epoch: {}, test loss: {:.3f}, test accuracy: {:.2f}%\".format(i+1, test_l, test_acc * 100))\n",
    "        '''\n",
    "        results.loc[i] = [l,acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNmodel(in_data):\n",
    "    input1d = tf.reshape(in_data, [-1,max_len,6])\n",
    "    conv11 = tf.layers.conv1d(\n",
    "        inputs=input1d, \n",
    "        filters=32, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv12 = tf.layers.conv1d(\n",
    "        inputs=conv11, \n",
    "        filters=32, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling1d(\n",
    "        inputs=conv12, \n",
    "        pool_size=10, \n",
    "        strides=10)\n",
    "    conv21 = tf.layers.conv1d(\n",
    "        inputs=pool1, \n",
    "        filters=64, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv211 = tf.layers.conv1d(\n",
    "        inputs=conv21, \n",
    "        filters=32, \n",
    "        kernel_size=1, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=None)\n",
    "    conv22 = tf.layers.conv1d(\n",
    "        inputs=conv211, \n",
    "        filters=64, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling1d(\n",
    "        inputs=conv22, \n",
    "        pool_size=10,\n",
    "        strides=10)\n",
    "    \n",
    "    conv31 = tf.layers.conv1d(\n",
    "        inputs=pool2, \n",
    "        filters=128, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv311 = tf.layers.conv1d(\n",
    "        inputs=conv31, \n",
    "        filters=64, \n",
    "        kernel_size=1, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv32 = tf.layers.conv1d(\n",
    "        inputs=conv311, \n",
    "        filters=128, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv321 = tf.layers.conv1d(\n",
    "        inputs=conv32, \n",
    "        filters=64, \n",
    "        kernel_size=1, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv33 = tf.layers.conv1d(\n",
    "        inputs=conv321, \n",
    "        filters=128, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling1d(\n",
    "        inputs=conv33, \n",
    "        pool_size=10, \n",
    "        strides=10)\n",
    "    \n",
    "    pool_flat = tf.layers.flatten(pool3)\n",
    "    fc = tf.layers.dense(\n",
    "        inputs= pool_flat, units=500)#, activation=tf.nn.relu)\n",
    "    #fc_drop = tf.nn.dropout(fc, keep_prob) \n",
    "    output = tf.layers.dense(inputs=fc, units=max_id+1) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
