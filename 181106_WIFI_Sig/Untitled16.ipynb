{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN1d_1(in_data):\n",
    "    input1d = tf.reshape(in_data, [-1,max_len,6])\n",
    "    conv1 = tf.layers.conv1d(\n",
    "        inputs=input1d, \n",
    "        filters=100, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        #padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling1d(\n",
    "        inputs=conv1, \n",
    "        pool_size=10, \n",
    "        strides=10)\n",
    "        #padding=\"same\")\n",
    "    conv2 = tf.layers.conv1d(\n",
    "        inputs=pool1, \n",
    "        filters=250, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        #padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling1d(\n",
    "        inputs=conv2, \n",
    "        pool_size=10, \n",
    "        strides=10)\n",
    "        #padding=\"same\")\n",
    "    pool_flat = tf.layers.flatten(pool2)\n",
    "    fc = tf.layers.dense(\n",
    "        inputs= pool_flat, units=500, activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(inputs=fc, units=max_id+1)    \n",
    "    return output\n",
    "\n",
    "\n",
    "def CNN1d_2(in_data):\n",
    "    input1d = tf.reshape(in_data, [-1,max_len,6])\n",
    "    conv11 = tf.layers.conv1d(\n",
    "        inputs=input1d, \n",
    "        filters=32, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv12 = tf.layers.conv1d(\n",
    "        inputs=conv11, \n",
    "        filters=32, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling1d(\n",
    "        inputs=conv12, \n",
    "        pool_size=10, \n",
    "        strides=10)\n",
    "    conv21 = tf.layers.conv1d(\n",
    "        inputs=pool1, \n",
    "        filters=64, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv211 = tf.layers.conv1d(\n",
    "        inputs=conv21, \n",
    "        filters=32, \n",
    "        kernel_size=1, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=None)\n",
    "    conv22 = tf.layers.conv1d(\n",
    "        inputs=conv211, \n",
    "        filters=64, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling1d(\n",
    "        inputs=conv22, \n",
    "        pool_size=10,\n",
    "        strides=10)\n",
    "    \n",
    "    conv31 = tf.layers.conv1d(\n",
    "        inputs=pool2, \n",
    "        filters=128, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv311 = tf.layers.conv1d(\n",
    "        inputs=conv31, \n",
    "        filters=64, \n",
    "        kernel_size=1, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv32 = tf.layers.conv1d(\n",
    "        inputs=conv311, \n",
    "        filters=128, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv321 = tf.layers.conv1d(\n",
    "        inputs=conv32, \n",
    "        filters=64, \n",
    "        kernel_size=1, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv33 = tf.layers.conv1d(\n",
    "        inputs=conv321, \n",
    "        filters=128, \n",
    "        kernel_size=3, \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling1d(\n",
    "        inputs=conv33, \n",
    "        pool_size=10, \n",
    "        strides=10)\n",
    "    \n",
    "    pool_flat = tf.layers.flatten(pool3)\n",
    "    fc = tf.layers.dense(\n",
    "        inputs= pool_flat, units=500)#, activation=tf.nn.relu)\n",
    "    #fc_drop = tf.nn.dropout(fc, keep_prob) \n",
    "    output = tf.layers.dense(inputs=fc, units=max_id+1) \n",
    "    return output\n",
    "\n",
    "def CNN2d(in_data):\n",
    "    input2d = tf.reshape(in_data, [-1,max_len,30,6])\n",
    "    conv11 = tf.layers.conv2d(\n",
    "        inputs=input2d, \n",
    "        filters=32, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv12 = tf.layers.conv2d(\n",
    "        inputs=conv11, \n",
    "        filters=32, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        inputs=conv12, \n",
    "        pool_size=[10, 1], \n",
    "        strides=2)\n",
    "    conv21 = tf.layers.conv2d(\n",
    "        inputs=pool1, \n",
    "        filters=64, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv211 = tf.layers.conv2d(\n",
    "        inputs=conv21, \n",
    "        filters=32, \n",
    "        kernel_size=[1, 1], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv22 = tf.layers.conv2d(\n",
    "        inputs=conv211, \n",
    "        filters=64, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        inputs=conv22, \n",
    "        pool_size=[10, 1], \n",
    "        strides=10)\n",
    "    \n",
    "    conv31 = tf.layers.conv2d(\n",
    "        inputs=pool2, \n",
    "        filters=128, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv311 = tf.layers.conv2d(\n",
    "        inputs=conv31, \n",
    "        filters=64, \n",
    "        kernel_size=[1, 1], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv32 = tf.layers.conv2d(\n",
    "        inputs=conv311, \n",
    "        filters=128, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv321 = tf.layers.conv2d(\n",
    "        inputs=conv32, \n",
    "        filters=64, \n",
    "        kernel_size=[1, 1], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv33 = tf.layers.conv2d(\n",
    "        inputs=conv321, \n",
    "        filters=128, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling2d(\n",
    "        inputs=conv33, \n",
    "        pool_size=[10, 1], \n",
    "        strides=10)\n",
    "    \n",
    "    pool_flat = tf.layers.flatten(pool3)\n",
    "    fc = tf.layers.dense(\n",
    "        inputs= pool_flat, units=500)#, activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(inputs=fc, units=max_id+1)    \n",
    "    return output\n",
    "\n",
    "def CNN4_JS(in_data):\n",
    "    input2d = tf.reshape(in_data, [-1,max_len,30,6])\n",
    "    conv11 = tf.layers.conv2d(\n",
    "        inputs=input2d, \n",
    "        filters=32, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=2,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv12 = tf.layers.conv2d(\n",
    "        inputs=conv11, \n",
    "        filters=64, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=2,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv21 = tf.layers.conv2d(\n",
    "        inputs=conv12, \n",
    "        filters=128, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv22 = tf.layers.conv2d(\n",
    "        inputs=conv21, \n",
    "        filters=256, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=2,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv31 = tf.layers.conv2d(\n",
    "        inputs=conv22, \n",
    "        filters=256, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv32 = tf.layers.conv2d(\n",
    "        inputs=conv31, \n",
    "        filters=256, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=2,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    conv41 = tf.layers.conv2d(\n",
    "        inputs=conv32, \n",
    "        filters=512, \n",
    "        kernel_size=[3, 3], \n",
    "        strides=1,\n",
    "        padding=\"same\", \n",
    "        activation=tf.nn.relu)\n",
    "    pool_flat = tf.layers.flatten(conv41)\n",
    "    fc = tf.layers.dense(\n",
    "        inputs= pool_flat, units=512)#, activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(inputs=fc, units=max_id+1)    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herok\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# data path\n",
    "path_csi =  'J:\\\\Data\\\\Wi-Fi_processed\\\\'\n",
    "path_csi_hc = 'J:\\\\Data\\\\Wi-Fi_HC\\\\180_100\\\\'\n",
    "\n",
    "# data info\n",
    "df_info = pd.read_csv('data_subc_sig.csv')\n",
    "\n",
    "# parameters\n",
    "max_id = np.max(df_info['id_person'])\n",
    "#max_len = int(np.max(df_info['len']))\n",
    "max_len = 500\n",
    "learning_rate = 0.1\n",
    "training_epochs = 800\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data generator\n",
    "def gen_csi(df_info,id_num,len_num):\n",
    "    file_list = os.listdir(path_csi_hc)\n",
    "    for file in file_list:\n",
    "        \n",
    "        data_read = np.load(path_csi_hc + file)\n",
    "        \n",
    "        data_xs = data_read[:,4:]\n",
    "        data_ys = data_read[:,:4]\n",
    "        data_y = data_read[:,0].astype('int')\n",
    "        data_y1h = np.eye(id_num+1)[data_y]\n",
    "        \n",
    "        for i in range(len(data_xs)):\n",
    "            yield(data_xs[i,:],data_y1h[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_idx,te_idx = train_test_split(df_info.index,test_size=0.2,random_state=10)\n",
    "#id_num = len(np.unique(df_info.id_person))\n",
    "df_train = df_info.loc[tr_idx]\n",
    "df_test = df_info.loc[te_idx]\n",
    "\n",
    "gen = lambda: (r for r in gen_csi(df_info,max_id,max_len))\n",
    "train_dataset = tf.data.Dataset().from_generator(gen, (tf.float32,tf.int32)).shuffle(100).repeat().batch(batch_size)\n",
    "\n",
    "# create general iterator\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# make datasets that we can initialize separately, but using the same structure via the common iterator\n",
    "training_init_op = iterator.make_initializer(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "logits = CNN4_JS(next_element[0])\n",
    "labels = next_element[1]\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=labels))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "with tf.name_scope('optimiser'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_pred'):\n",
    "        pred = tf.argmax(logits,1)\n",
    "        equal = tf.equal(pred,tf.argmax(labels,1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(equal,tf.float32))\n",
    "tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "#init\n",
    "init_op = tf.global_variables_initializer()\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "merged_summary_operation = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 5.077, training accuracy: 0.00%\n",
      "Epoch: 2, loss: 25452789891072.000, training accuracy: 0.00%\n",
      "Epoch: 3, loss: 115048734720.000, training accuracy: 0.00%\n",
      "Epoch: 4, loss: 1572396335104.000, training accuracy: 0.00%\n",
      "Epoch: 5, loss: 1129657270272.000, training accuracy: 0.00%\n",
      "Epoch: 6, loss: 110421712896.000, training accuracy: 10.00%\n",
      "Epoch: 7, loss: 60450676736.000, training accuracy: 10.00%\n",
      "Epoch: 8, loss: 565479604224.000, training accuracy: 0.00%\n",
      "Epoch: 9, loss: 13082981376.000, training accuracy: 10.00%\n",
      "Epoch: 10, loss: 42959151104.000, training accuracy: 0.00%\n",
      "Epoch: 11, loss: 50615250944.000, training accuracy: 0.00%\n",
      "Epoch: 12, loss: 14928103424.000, training accuracy: 0.00%\n",
      "Epoch: 13, loss: 310057088.000, training accuracy: 0.00%\n",
      "Epoch: 14, loss: 45466584.000, training accuracy: 0.00%\n",
      "Epoch: 15, loss: 19188784.000, training accuracy: 0.00%\n",
      "Epoch: 16, loss: 581522048.000, training accuracy: 0.00%\n",
      "Epoch: 17, loss: 326011392.000, training accuracy: 0.00%\n",
      "Epoch: 18, loss: 15262787584.000, training accuracy: 0.00%\n",
      "Epoch: 19, loss: 1067729216.000, training accuracy: 0.00%\n",
      "Epoch: 20, loss: 203241888.000, training accuracy: 0.00%\n",
      "Epoch: 21, loss: 66274920.000, training accuracy: 0.00%\n",
      "Epoch: 22, loss: 12181934.000, training accuracy: 0.00%\n",
      "Epoch: 23, loss: 8399395.000, training accuracy: 0.00%\n",
      "Epoch: 24, loss: 29664710.000, training accuracy: 0.00%\n",
      "Epoch: 25, loss: 96631400.000, training accuracy: 0.00%\n",
      "Epoch: 26, loss: 26987238.000, training accuracy: 0.00%\n",
      "Epoch: 27, loss: 25622732.000, training accuracy: 0.00%\n",
      "Epoch: 28, loss: 6808248.000, training accuracy: 10.00%\n",
      "Epoch: 29, loss: 3889608.000, training accuracy: 0.00%\n",
      "Epoch: 30, loss: 13560362.000, training accuracy: 0.00%\n",
      "Epoch: 31, loss: 5248825.000, training accuracy: 0.00%\n",
      "Epoch: 32, loss: 9510735.000, training accuracy: 0.00%\n",
      "Epoch: 33, loss: 24433376.000, training accuracy: 0.00%\n",
      "Epoch: 34, loss: 5233989.000, training accuracy: 0.00%\n",
      "Epoch: 35, loss: 7351877.500, training accuracy: 0.00%\n",
      "Epoch: 36, loss: 97767232.000, training accuracy: 0.00%\n",
      "Epoch: 37, loss: 17330772.000, training accuracy: 0.00%\n",
      "Epoch: 38, loss: 12247972.000, training accuracy: 0.00%\n",
      "Epoch: 39, loss: 9098190.000, training accuracy: 0.00%\n",
      "Epoch: 40, loss: 5698888.500, training accuracy: 0.00%\n",
      "Epoch: 41, loss: 1111938048.000, training accuracy: 10.00%\n",
      "Epoch: 42, loss: 22074532.000, training accuracy: 0.00%\n",
      "Epoch: 43, loss: 113920536.000, training accuracy: 0.00%\n",
      "Epoch: 44, loss: 1285958912.000, training accuracy: 0.00%\n",
      "Epoch: 45, loss: 405504896.000, training accuracy: 0.00%\n",
      "Epoch: 46, loss: 44391784.000, training accuracy: 0.00%\n",
      "Epoch: 47, loss: 45364520.000, training accuracy: 0.00%\n",
      "Epoch: 48, loss: 22506420.000, training accuracy: 0.00%\n",
      "Epoch: 49, loss: 4481844.500, training accuracy: 0.00%\n",
      "Epoch: 50, loss: 114976880.000, training accuracy: 0.00%\n",
      "Epoch: 51, loss: 10112457.000, training accuracy: 0.00%\n",
      "Epoch: 52, loss: 14100058.000, training accuracy: 0.00%\n",
      "Epoch: 53, loss: 336436640.000, training accuracy: 0.00%\n",
      "Epoch: 54, loss: 77260944.000, training accuracy: 0.00%\n",
      "Epoch: 55, loss: 1705840896.000, training accuracy: 0.00%\n",
      "Epoch: 56, loss: 116153552.000, training accuracy: 0.00%\n",
      "Epoch: 57, loss: 114089176.000, training accuracy: 10.00%\n",
      "Epoch: 58, loss: 7328215040.000, training accuracy: 0.00%\n",
      "Epoch: 59, loss: 75800480.000, training accuracy: 0.00%\n",
      "Epoch: 60, loss: 9997961.000, training accuracy: 0.00%\n",
      "Epoch: 61, loss: 652255680.000, training accuracy: 0.00%\n",
      "Epoch: 62, loss: 61300008.000, training accuracy: 0.00%\n",
      "Epoch: 63, loss: 818116992.000, training accuracy: 0.00%\n",
      "Epoch: 64, loss: 385745472.000, training accuracy: 0.00%\n",
      "Epoch: 65, loss: 342588992.000, training accuracy: 0.00%\n",
      "Epoch: 66, loss: 69404312.000, training accuracy: 0.00%\n",
      "Epoch: 67, loss: 143921504.000, training accuracy: 0.00%\n",
      "Epoch: 68, loss: 760747648.000, training accuracy: 0.00%\n",
      "Epoch: 69, loss: 22065760.000, training accuracy: 0.00%\n",
      "Epoch: 70, loss: 19216638.000, training accuracy: 0.00%\n",
      "Epoch: 71, loss: 11030550.000, training accuracy: 0.00%\n",
      "Epoch: 72, loss: 17017194.000, training accuracy: 0.00%\n",
      "Epoch: 73, loss: 13974696.000, training accuracy: 10.00%\n",
      "Epoch: 74, loss: 815972.625, training accuracy: 0.00%\n",
      "Epoch: 75, loss: 551944704.000, training accuracy: 0.00%\n",
      "Epoch: 76, loss: 12167198.000, training accuracy: 0.00%\n",
      "Epoch: 77, loss: 3169142272.000, training accuracy: 0.00%\n",
      "Epoch: 78, loss: 2304411136.000, training accuracy: 0.00%\n",
      "Epoch: 79, loss: 131522856.000, training accuracy: 0.00%\n",
      "Epoch: 80, loss: 630541440.000, training accuracy: 0.00%\n",
      "Epoch: 81, loss: 56890988.000, training accuracy: 0.00%\n",
      "Epoch: 82, loss: 422922560.000, training accuracy: 0.00%\n",
      "Epoch: 83, loss: 1314860800.000, training accuracy: 0.00%\n",
      "Epoch: 84, loss: 21846308.000, training accuracy: 10.00%\n",
      "Epoch: 85, loss: 152110512.000, training accuracy: 0.00%\n",
      "Epoch: 86, loss: 474949216.000, training accuracy: 0.00%\n",
      "Epoch: 87, loss: 1848544256.000, training accuracy: 0.00%\n",
      "Epoch: 88, loss: 1278288896.000, training accuracy: 0.00%\n",
      "Epoch: 89, loss: 1004901568.000, training accuracy: 0.00%\n",
      "Epoch: 90, loss: 262891648.000, training accuracy: 0.00%\n",
      "Epoch: 91, loss: 149466784.000, training accuracy: 0.00%\n",
      "Epoch: 92, loss: 132294848.000, training accuracy: 0.00%\n",
      "Epoch: 93, loss: 70738832.000, training accuracy: 0.00%\n",
      "Epoch: 94, loss: 21098032.000, training accuracy: 10.00%\n",
      "Epoch: 95, loss: 25474044.000, training accuracy: 10.00%\n",
      "Epoch: 96, loss: 338952.000, training accuracy: 10.00%\n",
      "Epoch: 97, loss: 375.237, training accuracy: 0.00%\n",
      "Epoch: 98, loss: 1083182720.000, training accuracy: 0.00%\n",
      "Epoch: 99, loss: 93233224.000, training accuracy: 0.00%\n",
      "Epoch: 100, loss: 46006328.000, training accuracy: 0.00%\n",
      "Epoch: 101, loss: 31093738.000, training accuracy: 0.00%\n",
      "Epoch: 102, loss: 234553936.000, training accuracy: 0.00%\n",
      "Epoch: 103, loss: 150868656.000, training accuracy: 0.00%\n",
      "Epoch: 104, loss: 23921790.000, training accuracy: 0.00%\n",
      "Epoch: 105, loss: 10366811.000, training accuracy: 0.00%\n",
      "Epoch: 106, loss: 4749680.500, training accuracy: 0.00%\n",
      "Epoch: 107, loss: 4771213.500, training accuracy: 0.00%\n",
      "Epoch: 108, loss: 25612828.000, training accuracy: 0.00%\n",
      "Epoch: 109, loss: 75389680.000, training accuracy: 0.00%\n",
      "Epoch: 110, loss: 849386.000, training accuracy: 0.00%\n",
      "Epoch: 111, loss: 170.945, training accuracy: 0.00%\n",
      "Epoch: 112, loss: 177.773, training accuracy: 0.00%\n",
      "Epoch: 113, loss: 203.416, training accuracy: 0.00%\n",
      "Epoch: 114, loss: 946616.875, training accuracy: 0.00%\n",
      "Epoch: 115, loss: 179.348, training accuracy: 10.00%\n",
      "Epoch: 116, loss: 171.506, training accuracy: 0.00%\n",
      "Epoch: 117, loss: 168.651, training accuracy: 0.00%\n",
      "Epoch: 118, loss: 2329835520.000, training accuracy: 0.00%\n",
      "Epoch: 119, loss: 183.744, training accuracy: 0.00%\n",
      "Epoch: 120, loss: 165.201, training accuracy: 0.00%\n",
      "Epoch: 121, loss: 155.813, training accuracy: 0.00%\n",
      "Epoch: 122, loss: 405244512.000, training accuracy: 0.00%\n",
      "Epoch: 123, loss: 209829.594, training accuracy: 0.00%\n",
      "Epoch: 124, loss: 573390720.000, training accuracy: 0.00%\n",
      "Epoch: 125, loss: 102.252, training accuracy: 0.00%\n",
      "Epoch: 126, loss: 3901.740, training accuracy: 0.00%\n",
      "Epoch: 127, loss: 890990.500, training accuracy: 10.00%\n",
      "Epoch: 128, loss: 2044912.750, training accuracy: 0.00%\n",
      "Epoch: 129, loss: 3450213.250, training accuracy: 10.00%\n",
      "Epoch: 130, loss: 2253665.500, training accuracy: 0.00%\n",
      "Epoch: 131, loss: 10099382.000, training accuracy: 0.00%\n",
      "Epoch: 132, loss: 1188299136.000, training accuracy: 0.00%\n",
      "Epoch: 133, loss: 663604864.000, training accuracy: 0.00%\n",
      "Epoch: 134, loss: 264384720.000, training accuracy: 0.00%\n",
      "Epoch: 135, loss: 47407212.000, training accuracy: 0.00%\n",
      "Epoch: 136, loss: 31928148.000, training accuracy: 0.00%\n",
      "Epoch: 137, loss: 907752960.000, training accuracy: 0.00%\n",
      "Epoch: 138, loss: 254167648.000, training accuracy: 0.00%\n",
      "Epoch: 139, loss: 190.273, training accuracy: 0.00%\n",
      "Epoch: 140, loss: 134.562, training accuracy: 0.00%\n",
      "Epoch: 141, loss: 1056079552.000, training accuracy: 0.00%\n",
      "Epoch: 142, loss: 189.246, training accuracy: 0.00%\n",
      "Epoch: 143, loss: 279.246, training accuracy: 0.00%\n",
      "Epoch: 144, loss: 351084224.000, training accuracy: 0.00%\n",
      "Epoch: 145, loss: 164.135, training accuracy: 0.00%\n",
      "Epoch: 146, loss: 40094728.000, training accuracy: 10.00%\n",
      "Epoch: 147, loss: 132.319, training accuracy: 0.00%\n",
      "Epoch: 148, loss: 123.448, training accuracy: 0.00%\n",
      "Epoch: 149, loss: 130.536, training accuracy: 0.00%\n",
      "Epoch: 150, loss: 79.472, training accuracy: 0.00%\n",
      "Epoch: 151, loss: 5707334.000, training accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152, loss: 47.993, training accuracy: 0.00%\n",
      "Epoch: 153, loss: 64.861, training accuracy: 0.00%\n",
      "Epoch: 154, loss: 69.811, training accuracy: 0.00%\n",
      "Epoch: 155, loss: 53.144, training accuracy: 0.00%\n",
      "Epoch: 156, loss: 49.784, training accuracy: 0.00%\n",
      "Epoch: 157, loss: 25186240.000, training accuracy: 0.00%\n",
      "Epoch: 158, loss: 161336480.000, training accuracy: 0.00%\n",
      "Epoch: 159, loss: 50.477, training accuracy: 10.00%\n",
      "Epoch: 160, loss: 52.114, training accuracy: 0.00%\n",
      "Epoch: 161, loss: 57.533, training accuracy: 0.00%\n",
      "Epoch: 162, loss: 54.774, training accuracy: 0.00%\n",
      "Epoch: 163, loss: 47.557, training accuracy: 0.00%\n",
      "Epoch: 164, loss: 57.076, training accuracy: 0.00%\n",
      "Epoch: 165, loss: 42.799, training accuracy: 0.00%\n",
      "Epoch: 166, loss: 35.951, training accuracy: 0.00%\n",
      "Epoch: 167, loss: 30.410, training accuracy: 0.00%\n",
      "Epoch: 168, loss: 28.536, training accuracy: 0.00%\n",
      "Epoch: 169, loss: 28.675, training accuracy: 0.00%\n",
      "Epoch: 170, loss: 19.697, training accuracy: 0.00%\n",
      "Epoch: 171, loss: 23.624, training accuracy: 0.00%\n",
      "Epoch: 172, loss: 35.012, training accuracy: 0.00%\n",
      "Epoch: 173, loss: 19026522112.000, training accuracy: 10.00%\n",
      "Epoch: 174, loss: 22.115, training accuracy: 0.00%\n",
      "Epoch: 175, loss: 24.407, training accuracy: 0.00%\n",
      "Epoch: 176, loss: 23.135, training accuracy: 10.00%\n",
      "Epoch: 177, loss: 61536600.000, training accuracy: 0.00%\n",
      "Epoch: 178, loss: 24.601, training accuracy: 0.00%\n",
      "Epoch: 179, loss: 27.463, training accuracy: 0.00%\n",
      "Epoch: 180, loss: 21.987, training accuracy: 0.00%\n",
      "Epoch: 181, loss: 30.997, training accuracy: 0.00%\n",
      "Epoch: 182, loss: 431872672.000, training accuracy: 0.00%\n",
      "Epoch: 183, loss: 38.276, training accuracy: 0.00%\n",
      "Epoch: 184, loss: 28.932, training accuracy: 0.00%\n",
      "Epoch: 185, loss: 41.803, training accuracy: 0.00%\n",
      "Epoch: 186, loss: 30.811, training accuracy: 0.00%\n",
      "Epoch: 187, loss: 60.330, training accuracy: 10.00%\n",
      "Epoch: 188, loss: 39.017, training accuracy: 0.00%\n",
      "Epoch: 189, loss: 37.962, training accuracy: 0.00%\n",
      "Epoch: 190, loss: 42.725, training accuracy: 0.00%\n",
      "Epoch: 191, loss: 27.540, training accuracy: 0.00%\n",
      "Epoch: 192, loss: 45.538, training accuracy: 0.00%\n",
      "Epoch: 193, loss: 26.772, training accuracy: 10.00%\n",
      "Epoch: 194, loss: 25.713, training accuracy: 0.00%\n",
      "Epoch: 195, loss: 20.129, training accuracy: 10.00%\n",
      "Epoch: 196, loss: 25.989, training accuracy: 10.00%\n",
      "Epoch: 197, loss: 22.263, training accuracy: 0.00%\n",
      "Epoch: 198, loss: 29.109, training accuracy: 10.00%\n",
      "Epoch: 199, loss: 22.864, training accuracy: 0.00%\n",
      "Epoch: 200, loss: 22.907, training accuracy: 0.00%\n",
      "Epoch: 201, loss: 22.131, training accuracy: 0.00%\n",
      "Epoch: 202, loss: 13.908, training accuracy: 0.00%\n",
      "Epoch: 203, loss: 12.819, training accuracy: 0.00%\n",
      "Epoch: 204, loss: 31.102, training accuracy: 0.00%\n",
      "Epoch: 205, loss: 19.170, training accuracy: 10.00%\n",
      "Epoch: 206, loss: 13.075, training accuracy: 10.00%\n",
      "Epoch: 207, loss: 14.700, training accuracy: 0.00%\n",
      "Epoch: 208, loss: 7.034, training accuracy: 0.00%\n",
      "Epoch: 209, loss: 12.099, training accuracy: 0.00%\n",
      "Epoch: 210, loss: 8.102, training accuracy: 0.00%\n",
      "Epoch: 211, loss: 7.298, training accuracy: 0.00%\n",
      "Epoch: 212, loss: 8.130, training accuracy: 10.00%\n",
      "Epoch: 213, loss: 11.828, training accuracy: 0.00%\n",
      "Epoch: 214, loss: 7.353, training accuracy: 0.00%\n",
      "Epoch: 215, loss: 13.360, training accuracy: 0.00%\n",
      "Epoch: 216, loss: 8.868, training accuracy: 0.00%\n",
      "Epoch: 217, loss: 7.018, training accuracy: 0.00%\n",
      "Epoch: 218, loss: 6.263, training accuracy: 10.00%\n",
      "Epoch: 219, loss: 9.917, training accuracy: 0.00%\n",
      "Epoch: 220, loss: 10.003, training accuracy: 0.00%\n",
      "Epoch: 221, loss: 18.965, training accuracy: 0.00%\n",
      "Epoch: 222, loss: 19.212, training accuracy: 10.00%\n",
      "Epoch: 223, loss: 20.314, training accuracy: 0.00%\n",
      "Epoch: 224, loss: 22.501, training accuracy: 10.00%\n",
      "Epoch: 225, loss: 17.555, training accuracy: 0.00%\n",
      "Epoch: 226, loss: 26.489, training accuracy: 0.00%\n",
      "Epoch: 227, loss: 28.237, training accuracy: 0.00%\n",
      "Epoch: 228, loss: 22.042, training accuracy: 0.00%\n",
      "Epoch: 229, loss: 20.785, training accuracy: 0.00%\n",
      "Epoch: 230, loss: 18.496, training accuracy: 10.00%\n",
      "Epoch: 231, loss: 14.199, training accuracy: 0.00%\n",
      "Epoch: 232, loss: 26.972, training accuracy: 0.00%\n",
      "Epoch: 233, loss: 23.824, training accuracy: 0.00%\n",
      "Epoch: 234, loss: 25.538, training accuracy: 0.00%\n",
      "Epoch: 235, loss: 21.572, training accuracy: 0.00%\n",
      "Epoch: 236, loss: 18.628, training accuracy: 0.00%\n",
      "Epoch: 237, loss: 24.517, training accuracy: 0.00%\n",
      "Epoch: 238, loss: 23.465, training accuracy: 0.00%\n",
      "Epoch: 239, loss: 31.577, training accuracy: 0.00%\n",
      "Epoch: 240, loss: 43.237, training accuracy: 10.00%\n",
      "Epoch: 241, loss: 22.205, training accuracy: 0.00%\n",
      "Epoch: 242, loss: 25.035, training accuracy: 10.00%\n",
      "Epoch: 243, loss: 21.657, training accuracy: 0.00%\n",
      "Epoch: 244, loss: 14.504, training accuracy: 0.00%\n",
      "Epoch: 245, loss: 25.417, training accuracy: 0.00%\n",
      "Epoch: 246, loss: 14.901, training accuracy: 0.00%\n",
      "Epoch: 247, loss: 20.500, training accuracy: 0.00%\n",
      "Epoch: 248, loss: 10.906, training accuracy: 0.00%\n",
      "Epoch: 249, loss: 7.453, training accuracy: 0.00%\n",
      "Epoch: 250, loss: 9.491, training accuracy: 0.00%\n",
      "Epoch: 251, loss: 9.456, training accuracy: 0.00%\n",
      "Epoch: 252, loss: 7.923, training accuracy: 0.00%\n",
      "Epoch: 253, loss: 6.731, training accuracy: 0.00%\n",
      "Epoch: 254, loss: 8.551, training accuracy: 0.00%\n",
      "Epoch: 255, loss: 8.474, training accuracy: 0.00%\n",
      "Epoch: 256, loss: 15.021, training accuracy: 0.00%\n",
      "Epoch: 257, loss: 8.358, training accuracy: 0.00%\n",
      "Epoch: 258, loss: 7.897, training accuracy: 0.00%\n",
      "Epoch: 259, loss: 6.769, training accuracy: 0.00%\n",
      "Epoch: 260, loss: 4.173, training accuracy: 0.00%\n",
      "Epoch: 261, loss: 6.338, training accuracy: 0.00%\n",
      "Epoch: 262, loss: 9.386, training accuracy: 0.00%\n",
      "Epoch: 263, loss: 12.753, training accuracy: 10.00%\n",
      "Epoch: 264, loss: 13.405, training accuracy: 0.00%\n",
      "Epoch: 265, loss: 10.615, training accuracy: 0.00%\n",
      "Epoch: 266, loss: 15.376, training accuracy: 0.00%\n",
      "Epoch: 267, loss: 7.704, training accuracy: 0.00%\n",
      "Epoch: 268, loss: 11.957, training accuracy: 0.00%\n",
      "Epoch: 269, loss: 7.388, training accuracy: 0.00%\n",
      "Epoch: 270, loss: 9.876, training accuracy: 0.00%\n",
      "Epoch: 271, loss: 9.423, training accuracy: 0.00%\n",
      "Epoch: 272, loss: 13.544, training accuracy: 0.00%\n",
      "Epoch: 273, loss: 11.655, training accuracy: 0.00%\n",
      "Epoch: 274, loss: 9.777, training accuracy: 0.00%\n",
      "Epoch: 275, loss: 13.691, training accuracy: 0.00%\n",
      "Epoch: 276, loss: 14.170, training accuracy: 0.00%\n",
      "Epoch: 277, loss: 11.972, training accuracy: 0.00%\n",
      "Epoch: 278, loss: 14.406, training accuracy: 0.00%\n",
      "Epoch: 279, loss: 19.397, training accuracy: 0.00%\n",
      "Epoch: 280, loss: 13.108, training accuracy: 0.00%\n",
      "Epoch: 281, loss: 11.944, training accuracy: 0.00%\n",
      "Epoch: 282, loss: 16.087, training accuracy: 10.00%\n",
      "Epoch: 283, loss: 18.069, training accuracy: 0.00%\n",
      "Epoch: 284, loss: 17.748, training accuracy: 10.00%\n",
      "Epoch: 285, loss: 16.683, training accuracy: 0.00%\n",
      "Epoch: 286, loss: 15.363, training accuracy: 10.00%\n",
      "Epoch: 287, loss: 22.530, training accuracy: 0.00%\n",
      "Epoch: 288, loss: 20.532, training accuracy: 0.00%\n",
      "Epoch: 289, loss: 19.216, training accuracy: 0.00%\n",
      "Epoch: 290, loss: 10.936, training accuracy: 0.00%\n",
      "Epoch: 291, loss: 13.708, training accuracy: 0.00%\n",
      "Epoch: 292, loss: 24.790, training accuracy: 0.00%\n",
      "Epoch: 293, loss: 28.353, training accuracy: 0.00%\n",
      "Epoch: 294, loss: 6.574, training accuracy: 20.00%\n",
      "Epoch: 295, loss: 15.083, training accuracy: 0.00%\n",
      "Epoch: 296, loss: 18.870, training accuracy: 0.00%\n",
      "Epoch: 297, loss: 15.311, training accuracy: 0.00%\n",
      "Epoch: 298, loss: 18.512, training accuracy: 0.00%\n",
      "Epoch: 299, loss: 17.579, training accuracy: 0.00%\n",
      "Epoch: 300, loss: 12.117, training accuracy: 0.00%\n",
      "Epoch: 301, loss: 23.838, training accuracy: 0.00%\n",
      "Epoch: 302, loss: 8.065, training accuracy: 20.00%\n",
      "Epoch: 303, loss: 10.220, training accuracy: 10.00%\n",
      "Epoch: 304, loss: 13.425, training accuracy: 0.00%\n",
      "Epoch: 305, loss: 20.616, training accuracy: 0.00%\n",
      "Epoch: 306, loss: 17.327, training accuracy: 0.00%\n",
      "Epoch: 307, loss: 14.988, training accuracy: 0.00%\n",
      "Epoch: 308, loss: 13.773, training accuracy: 0.00%\n",
      "Epoch: 309, loss: 10.654, training accuracy: 0.00%\n",
      "Epoch: 310, loss: 5.663, training accuracy: 0.00%\n",
      "Epoch: 311, loss: 12.289, training accuracy: 10.00%\n",
      "Epoch: 312, loss: 7.134, training accuracy: 10.00%\n",
      "Epoch: 313, loss: 20.149, training accuracy: 20.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314, loss: 17.743, training accuracy: 0.00%\n",
      "Epoch: 315, loss: 26.773, training accuracy: 0.00%\n",
      "Epoch: 316, loss: 26.830, training accuracy: 0.00%\n",
      "Epoch: 317, loss: 11.242, training accuracy: 10.00%\n",
      "Epoch: 318, loss: 10.376, training accuracy: 0.00%\n",
      "Epoch: 319, loss: 11.371, training accuracy: 0.00%\n",
      "Epoch: 320, loss: 14.801, training accuracy: 0.00%\n",
      "Epoch: 321, loss: 14.061, training accuracy: 0.00%\n",
      "Epoch: 322, loss: 21.262, training accuracy: 0.00%\n",
      "Epoch: 323, loss: 17.909, training accuracy: 0.00%\n",
      "Epoch: 324, loss: 8.899, training accuracy: 0.00%\n",
      "Epoch: 325, loss: 13.528, training accuracy: 0.00%\n",
      "Epoch: 326, loss: 18.285, training accuracy: 0.00%\n",
      "Epoch: 327, loss: 25.175, training accuracy: 0.00%\n",
      "Epoch: 328, loss: 12.385, training accuracy: 0.00%\n",
      "Epoch: 329, loss: 11.863, training accuracy: 0.00%\n",
      "Epoch: 330, loss: 10.190, training accuracy: 0.00%\n",
      "Epoch: 331, loss: 27.649, training accuracy: 0.00%\n",
      "Epoch: 332, loss: 25.659, training accuracy: 0.00%\n",
      "Epoch: 333, loss: 19.517, training accuracy: 10.00%\n",
      "Epoch: 334, loss: 24.755, training accuracy: 0.00%\n",
      "Epoch: 335, loss: 24.493, training accuracy: 0.00%\n",
      "Epoch: 336, loss: 19.320, training accuracy: 0.00%\n",
      "Epoch: 337, loss: 16.896, training accuracy: 0.00%\n",
      "Epoch: 338, loss: 21.707, training accuracy: 0.00%\n",
      "Epoch: 339, loss: 22.035, training accuracy: 0.00%\n",
      "Epoch: 340, loss: 11.041, training accuracy: 0.00%\n",
      "Epoch: 341, loss: 22.286, training accuracy: 0.00%\n",
      "Epoch: 342, loss: 10.208, training accuracy: 0.00%\n",
      "Epoch: 343, loss: 23.591, training accuracy: 0.00%\n",
      "Epoch: 344, loss: 20.806, training accuracy: 0.00%\n",
      "Epoch: 345, loss: 21.918, training accuracy: 0.00%\n",
      "Epoch: 346, loss: 21.063, training accuracy: 0.00%\n",
      "Epoch: 347, loss: 12.689, training accuracy: 0.00%\n",
      "Epoch: 348, loss: 10.781, training accuracy: 0.00%\n",
      "Epoch: 349, loss: 11.901, training accuracy: 10.00%\n",
      "Epoch: 350, loss: 10.606, training accuracy: 0.00%\n",
      "Epoch: 351, loss: 12.906, training accuracy: 10.00%\n",
      "Epoch: 352, loss: 9.005, training accuracy: 0.00%\n",
      "Epoch: 353, loss: 15.322, training accuracy: 0.00%\n",
      "Epoch: 354, loss: 17.149, training accuracy: 10.00%\n",
      "Epoch: 355, loss: 13.160, training accuracy: 0.00%\n",
      "Epoch: 356, loss: 10.526, training accuracy: 0.00%\n",
      "Epoch: 357, loss: 13.258, training accuracy: 0.00%\n",
      "Epoch: 358, loss: 13.058, training accuracy: 0.00%\n",
      "Epoch: 359, loss: 10.670, training accuracy: 0.00%\n",
      "Epoch: 360, loss: 7.511, training accuracy: 0.00%\n",
      "Epoch: 361, loss: 5.811, training accuracy: 0.00%\n",
      "Epoch: 362, loss: 6.849, training accuracy: 0.00%\n",
      "Epoch: 363, loss: 13.861, training accuracy: 10.00%\n",
      "Epoch: 364, loss: 18.566, training accuracy: 0.00%\n",
      "Epoch: 365, loss: 11.586, training accuracy: 0.00%\n",
      "Epoch: 366, loss: 5.927, training accuracy: 0.00%\n",
      "Epoch: 367, loss: 10.487, training accuracy: 0.00%\n",
      "Epoch: 368, loss: 7.139, training accuracy: 0.00%\n",
      "Epoch: 369, loss: 7.098, training accuracy: 0.00%\n",
      "Epoch: 370, loss: 10.183, training accuracy: 0.00%\n",
      "Epoch: 371, loss: 11.137, training accuracy: 10.00%\n",
      "Epoch: 372, loss: 13.245, training accuracy: 10.00%\n",
      "Epoch: 373, loss: 17.826, training accuracy: 0.00%\n",
      "Epoch: 374, loss: 12.166, training accuracy: 0.00%\n",
      "Epoch: 375, loss: 13.509, training accuracy: 0.00%\n",
      "Epoch: 376, loss: 14.275, training accuracy: 0.00%\n",
      "Epoch: 377, loss: 24.135, training accuracy: 0.00%\n",
      "Epoch: 378, loss: 12.196, training accuracy: 10.00%\n",
      "Epoch: 379, loss: 23.728, training accuracy: 0.00%\n",
      "Epoch: 380, loss: 24.491, training accuracy: 0.00%\n",
      "Epoch: 381, loss: 14.827, training accuracy: 10.00%\n",
      "Epoch: 382, loss: 29.682, training accuracy: 0.00%\n",
      "Epoch: 383, loss: 20.956, training accuracy: 0.00%\n",
      "Epoch: 384, loss: 24.228, training accuracy: 0.00%\n",
      "Epoch: 385, loss: 25.390, training accuracy: 10.00%\n",
      "Epoch: 386, loss: 12.945, training accuracy: 10.00%\n",
      "Epoch: 387, loss: 18.097, training accuracy: 0.00%\n",
      "Epoch: 388, loss: 20.838, training accuracy: 0.00%\n",
      "Epoch: 389, loss: 30.194, training accuracy: 0.00%\n",
      "Epoch: 390, loss: 20.471, training accuracy: 0.00%\n",
      "Epoch: 391, loss: 17.709, training accuracy: 0.00%\n",
      "Epoch: 392, loss: 14.278, training accuracy: 0.00%\n",
      "Epoch: 393, loss: 8.621, training accuracy: 20.00%\n",
      "Epoch: 394, loss: 13.111, training accuracy: 0.00%\n",
      "Epoch: 395, loss: 12.700, training accuracy: 0.00%\n",
      "Epoch: 396, loss: 14.123, training accuracy: 0.00%\n",
      "Epoch: 397, loss: 13.303, training accuracy: 0.00%\n",
      "Epoch: 398, loss: 7.872, training accuracy: 0.00%\n",
      "Epoch: 399, loss: 15.282, training accuracy: 0.00%\n",
      "Epoch: 400, loss: 15.924, training accuracy: 10.00%\n",
      "Epoch: 401, loss: 29.198, training accuracy: 0.00%\n",
      "Epoch: 402, loss: 20.936, training accuracy: 0.00%\n",
      "Epoch: 403, loss: 11.990, training accuracy: 0.00%\n",
      "Epoch: 404, loss: 8.061, training accuracy: 0.00%\n",
      "Epoch: 405, loss: 12.745, training accuracy: 0.00%\n",
      "Epoch: 406, loss: 13.029, training accuracy: 0.00%\n",
      "Epoch: 407, loss: 29.557, training accuracy: 0.00%\n",
      "Epoch: 408, loss: 20.651, training accuracy: 10.00%\n",
      "Epoch: 409, loss: 22.841, training accuracy: 0.00%\n",
      "Epoch: 410, loss: 15.158, training accuracy: 0.00%\n",
      "Epoch: 411, loss: 12.374, training accuracy: 0.00%\n",
      "Epoch: 412, loss: 13.077, training accuracy: 0.00%\n",
      "Epoch: 413, loss: 10.949, training accuracy: 0.00%\n",
      "Epoch: 414, loss: 10.874, training accuracy: 0.00%\n",
      "Epoch: 415, loss: 10.954, training accuracy: 0.00%\n",
      "Epoch: 416, loss: 7.577, training accuracy: 10.00%\n",
      "Epoch: 417, loss: 9.163, training accuracy: 0.00%\n",
      "Epoch: 418, loss: 8.147, training accuracy: 0.00%\n",
      "Epoch: 419, loss: 10.416, training accuracy: 0.00%\n",
      "Epoch: 420, loss: 6.190, training accuracy: 0.00%\n",
      "Epoch: 421, loss: 13.259, training accuracy: 0.00%\n",
      "Epoch: 422, loss: 17.617, training accuracy: 0.00%\n",
      "Epoch: 423, loss: 19.614, training accuracy: 0.00%\n",
      "Epoch: 424, loss: 25.125, training accuracy: 0.00%\n",
      "Epoch: 425, loss: 11.302, training accuracy: 0.00%\n",
      "Epoch: 426, loss: 22.942, training accuracy: 0.00%\n",
      "Epoch: 427, loss: 20.672, training accuracy: 0.00%\n",
      "Epoch: 428, loss: 21.275, training accuracy: 0.00%\n",
      "Epoch: 429, loss: 16.947, training accuracy: 10.00%\n",
      "Epoch: 430, loss: 16.465, training accuracy: 0.00%\n",
      "Epoch: 431, loss: 21.650, training accuracy: 0.00%\n",
      "Epoch: 432, loss: 20.131, training accuracy: 0.00%\n",
      "Epoch: 433, loss: 25.266, training accuracy: 0.00%\n",
      "Epoch: 434, loss: 29.109, training accuracy: 0.00%\n",
      "Epoch: 435, loss: 25.717, training accuracy: 0.00%\n",
      "Epoch: 436, loss: 31.628, training accuracy: 0.00%\n",
      "Epoch: 437, loss: 35.764, training accuracy: 0.00%\n",
      "Epoch: 438, loss: 26.792, training accuracy: 0.00%\n",
      "Epoch: 439, loss: 24.951, training accuracy: 10.00%\n",
      "Epoch: 440, loss: 30.206, training accuracy: 0.00%\n",
      "Epoch: 441, loss: 14.668, training accuracy: 0.00%\n",
      "Epoch: 442, loss: 18.353, training accuracy: 0.00%\n",
      "Epoch: 443, loss: 20.660, training accuracy: 0.00%\n",
      "Epoch: 444, loss: 8.360, training accuracy: 20.00%\n",
      "Epoch: 445, loss: 14.988, training accuracy: 0.00%\n",
      "Epoch: 446, loss: 11.754, training accuracy: 10.00%\n",
      "Epoch: 447, loss: 11.718, training accuracy: 0.00%\n",
      "Epoch: 448, loss: 10.011, training accuracy: 0.00%\n",
      "Epoch: 449, loss: 19.066, training accuracy: 0.00%\n",
      "Epoch: 450, loss: 14.896, training accuracy: 0.00%\n",
      "Epoch: 451, loss: 13.511, training accuracy: 0.00%\n",
      "Epoch: 452, loss: 8.633, training accuracy: 0.00%\n",
      "Epoch: 453, loss: 12.213, training accuracy: 0.00%\n",
      "Epoch: 454, loss: 10.067, training accuracy: 0.00%\n",
      "Epoch: 455, loss: 10.387, training accuracy: 0.00%\n",
      "Epoch: 456, loss: 10.851, training accuracy: 0.00%\n",
      "Epoch: 457, loss: 10.222, training accuracy: 0.00%\n",
      "Epoch: 458, loss: 9.377, training accuracy: 0.00%\n",
      "Epoch: 459, loss: 8.451, training accuracy: 0.00%\n",
      "Epoch: 460, loss: 5.920, training accuracy: 0.00%\n",
      "Epoch: 461, loss: 7.292, training accuracy: 0.00%\n",
      "Epoch: 462, loss: 8.921, training accuracy: 20.00%\n",
      "Epoch: 463, loss: 16.109, training accuracy: 0.00%\n",
      "Epoch: 464, loss: 9.331, training accuracy: 10.00%\n",
      "Epoch: 465, loss: 10.454, training accuracy: 0.00%\n",
      "Epoch: 466, loss: 10.446, training accuracy: 0.00%\n",
      "Epoch: 467, loss: 5.228, training accuracy: 0.00%\n",
      "Epoch: 468, loss: 8.963, training accuracy: 0.00%\n",
      "Epoch: 469, loss: 12.228, training accuracy: 0.00%\n",
      "Epoch: 470, loss: 6.558, training accuracy: 20.00%\n",
      "Epoch: 471, loss: 12.066, training accuracy: 10.00%\n",
      "Epoch: 472, loss: 10.596, training accuracy: 0.00%\n",
      "Epoch: 473, loss: 9.359, training accuracy: 0.00%\n",
      "Epoch: 474, loss: 12.385, training accuracy: 0.00%\n",
      "Epoch: 475, loss: 14.732, training accuracy: 0.00%\n",
      "Epoch: 476, loss: 11.022, training accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477, loss: 23.436, training accuracy: 0.00%\n",
      "Epoch: 478, loss: 16.665, training accuracy: 0.00%\n",
      "Epoch: 479, loss: 17.413, training accuracy: 0.00%\n",
      "Epoch: 480, loss: 21.722, training accuracy: 0.00%\n",
      "Epoch: 481, loss: 17.469, training accuracy: 0.00%\n",
      "Epoch: 482, loss: 26.169, training accuracy: 10.00%\n",
      "Epoch: 483, loss: 19.453, training accuracy: 0.00%\n",
      "Epoch: 484, loss: 24.460, training accuracy: 0.00%\n",
      "Epoch: 485, loss: 16.728, training accuracy: 0.00%\n",
      "Epoch: 486, loss: 19.690, training accuracy: 0.00%\n",
      "Epoch: 487, loss: 21.020, training accuracy: 0.00%\n",
      "Epoch: 488, loss: 16.931, training accuracy: 0.00%\n",
      "Epoch: 489, loss: 20.605, training accuracy: 0.00%\n",
      "Epoch: 490, loss: 25.239, training accuracy: 0.00%\n",
      "Epoch: 491, loss: 15.685, training accuracy: 0.00%\n",
      "Epoch: 492, loss: 16.243, training accuracy: 0.00%\n",
      "Epoch: 493, loss: 11.964, training accuracy: 0.00%\n",
      "Epoch: 494, loss: 19.923, training accuracy: 0.00%\n",
      "Epoch: 495, loss: 14.574, training accuracy: 0.00%\n",
      "Epoch: 496, loss: 18.871, training accuracy: 0.00%\n",
      "Epoch: 497, loss: 16.118, training accuracy: 0.00%\n",
      "Epoch: 498, loss: 16.434, training accuracy: 0.00%\n",
      "Epoch: 499, loss: 11.490, training accuracy: 0.00%\n",
      "Epoch: 500, loss: 13.212, training accuracy: 0.00%\n",
      "Epoch: 501, loss: 10.530, training accuracy: 0.00%\n",
      "Epoch: 502, loss: 13.982, training accuracy: 0.00%\n",
      "Epoch: 503, loss: 18.421, training accuracy: 10.00%\n",
      "Epoch: 504, loss: 18.262, training accuracy: 0.00%\n",
      "Epoch: 505, loss: 17.888, training accuracy: 0.00%\n",
      "Epoch: 506, loss: 9.637, training accuracy: 0.00%\n",
      "Epoch: 507, loss: 10.869, training accuracy: 0.00%\n",
      "Epoch: 508, loss: 9.666, training accuracy: 0.00%\n",
      "Epoch: 509, loss: 8.272, training accuracy: 0.00%\n",
      "Epoch: 510, loss: 8.529, training accuracy: 0.00%\n",
      "Epoch: 511, loss: 8.205, training accuracy: 10.00%\n",
      "Epoch: 512, loss: 12.302, training accuracy: 0.00%\n",
      "Epoch: 513, loss: 11.897, training accuracy: 10.00%\n",
      "Epoch: 514, loss: 9.039, training accuracy: 0.00%\n",
      "Epoch: 515, loss: 8.777, training accuracy: 0.00%\n",
      "Epoch: 516, loss: 14.470, training accuracy: 0.00%\n",
      "Epoch: 517, loss: 8.667, training accuracy: 0.00%\n",
      "Epoch: 518, loss: 8.792, training accuracy: 0.00%\n",
      "Epoch: 519, loss: 5.465, training accuracy: 0.00%\n",
      "Epoch: 520, loss: 13.156, training accuracy: 0.00%\n",
      "Epoch: 521, loss: 13.153, training accuracy: 0.00%\n",
      "Epoch: 522, loss: 6.704, training accuracy: 0.00%\n",
      "Epoch: 523, loss: 9.260, training accuracy: 20.00%\n",
      "Epoch: 524, loss: 18.396, training accuracy: 0.00%\n",
      "Epoch: 525, loss: 16.332, training accuracy: 0.00%\n",
      "Epoch: 526, loss: 15.127, training accuracy: 0.00%\n",
      "Epoch: 527, loss: 12.531, training accuracy: 0.00%\n",
      "Epoch: 528, loss: 13.277, training accuracy: 0.00%\n",
      "Epoch: 529, loss: 15.636, training accuracy: 0.00%\n",
      "Epoch: 530, loss: 18.149, training accuracy: 0.00%\n",
      "Epoch: 531, loss: 19.401, training accuracy: 0.00%\n",
      "Epoch: 532, loss: 25.482, training accuracy: 0.00%\n",
      "Epoch: 533, loss: 20.880, training accuracy: 0.00%\n",
      "Epoch: 534, loss: 23.132, training accuracy: 10.00%\n",
      "Epoch: 535, loss: 13.205, training accuracy: 0.00%\n",
      "Epoch: 536, loss: 17.658, training accuracy: 0.00%\n",
      "Epoch: 537, loss: 32.671, training accuracy: 0.00%\n",
      "Epoch: 538, loss: 31.384, training accuracy: 0.00%\n",
      "Epoch: 539, loss: 25.347, training accuracy: 0.00%\n",
      "Epoch: 540, loss: 23.868, training accuracy: 0.00%\n",
      "Epoch: 541, loss: 19.851, training accuracy: 0.00%\n",
      "Epoch: 542, loss: 20.554, training accuracy: 10.00%\n",
      "Epoch: 543, loss: 24.401, training accuracy: 0.00%\n",
      "Epoch: 544, loss: 23.837, training accuracy: 0.00%\n",
      "Epoch: 545, loss: 16.245, training accuracy: 0.00%\n",
      "Epoch: 546, loss: 14.618, training accuracy: 0.00%\n",
      "Epoch: 547, loss: 16.776, training accuracy: 0.00%\n",
      "Epoch: 548, loss: 12.811, training accuracy: 0.00%\n",
      "Epoch: 549, loss: 18.160, training accuracy: 0.00%\n",
      "Epoch: 550, loss: 11.830, training accuracy: 10.00%\n",
      "Epoch: 551, loss: 10.709, training accuracy: 0.00%\n",
      "Epoch: 552, loss: 12.359, training accuracy: 10.00%\n",
      "Epoch: 553, loss: 15.356, training accuracy: 10.00%\n",
      "Epoch: 554, loss: 9.406, training accuracy: 0.00%\n",
      "Epoch: 555, loss: 7.786, training accuracy: 10.00%\n",
      "Epoch: 556, loss: 5.610, training accuracy: 0.00%\n",
      "Epoch: 557, loss: 5.735, training accuracy: 20.00%\n",
      "Epoch: 558, loss: 7.422, training accuracy: 0.00%\n",
      "Epoch: 559, loss: 6.808, training accuracy: 10.00%\n",
      "Epoch: 560, loss: 4.957, training accuracy: 10.00%\n",
      "Epoch: 561, loss: 6.881, training accuracy: 0.00%\n",
      "Epoch: 562, loss: 6.530, training accuracy: 20.00%\n",
      "Epoch: 563, loss: 8.832, training accuracy: 0.00%\n",
      "Epoch: 564, loss: 9.493, training accuracy: 0.00%\n",
      "Epoch: 565, loss: 13.169, training accuracy: 0.00%\n",
      "Epoch: 566, loss: 12.467, training accuracy: 0.00%\n",
      "Epoch: 567, loss: 11.162, training accuracy: 0.00%\n",
      "Epoch: 568, loss: 12.574, training accuracy: 0.00%\n",
      "Epoch: 569, loss: 7.309, training accuracy: 0.00%\n",
      "Epoch: 570, loss: 10.764, training accuracy: 0.00%\n",
      "Epoch: 571, loss: 9.639, training accuracy: 0.00%\n",
      "Epoch: 572, loss: 8.824, training accuracy: 0.00%\n",
      "Epoch: 573, loss: 9.054, training accuracy: 10.00%\n",
      "Epoch: 574, loss: 7.919, training accuracy: 0.00%\n",
      "Epoch: 575, loss: 10.090, training accuracy: 0.00%\n",
      "Epoch: 576, loss: 10.876, training accuracy: 0.00%\n",
      "Epoch: 577, loss: 13.358, training accuracy: 0.00%\n",
      "Epoch: 578, loss: 12.845, training accuracy: 10.00%\n",
      "Epoch: 579, loss: 16.451, training accuracy: 0.00%\n",
      "Epoch: 580, loss: 16.461, training accuracy: 10.00%\n",
      "Epoch: 581, loss: 21.665, training accuracy: 0.00%\n",
      "Epoch: 582, loss: 29.293, training accuracy: 0.00%\n",
      "Epoch: 583, loss: 27.601, training accuracy: 0.00%\n",
      "Epoch: 584, loss: 20.308, training accuracy: 0.00%\n",
      "Epoch: 585, loss: 16.989, training accuracy: 0.00%\n",
      "Epoch: 586, loss: 13.162, training accuracy: 10.00%\n",
      "Epoch: 587, loss: 23.487, training accuracy: 0.00%\n",
      "Epoch: 588, loss: 19.701, training accuracy: 0.00%\n",
      "Epoch: 589, loss: 19.064, training accuracy: 10.00%\n",
      "Epoch: 590, loss: 19.088, training accuracy: 0.00%\n",
      "Epoch: 591, loss: 15.038, training accuracy: 0.00%\n",
      "Epoch: 592, loss: 14.295, training accuracy: 10.00%\n",
      "Epoch: 593, loss: 13.465, training accuracy: 0.00%\n",
      "Epoch: 594, loss: 24.201, training accuracy: 0.00%\n",
      "Epoch: 595, loss: 11.507, training accuracy: 0.00%\n",
      "Epoch: 596, loss: 14.566, training accuracy: 0.00%\n",
      "Epoch: 597, loss: 13.231, training accuracy: 10.00%\n",
      "Epoch: 598, loss: 14.033, training accuracy: 0.00%\n",
      "Epoch: 599, loss: 9.451, training accuracy: 0.00%\n",
      "Epoch: 600, loss: 5.449, training accuracy: 0.00%\n",
      "Epoch: 601, loss: 22.945, training accuracy: 0.00%\n",
      "Epoch: 602, loss: 8.403, training accuracy: 10.00%\n",
      "Epoch: 603, loss: 11.650, training accuracy: 0.00%\n",
      "Epoch: 604, loss: 10.890, training accuracy: 0.00%\n",
      "Epoch: 605, loss: 8.332, training accuracy: 0.00%\n",
      "Epoch: 606, loss: 9.272, training accuracy: 0.00%\n",
      "Epoch: 607, loss: 8.411, training accuracy: 0.00%\n",
      "Epoch: 608, loss: 10.632, training accuracy: 0.00%\n",
      "Epoch: 609, loss: 9.880, training accuracy: 0.00%\n",
      "Epoch: 610, loss: 4.307, training accuracy: 0.00%\n",
      "Epoch: 611, loss: 5.554, training accuracy: 0.00%\n",
      "Epoch: 612, loss: 6.066, training accuracy: 0.00%\n",
      "Epoch: 613, loss: 6.566, training accuracy: 0.00%\n",
      "Epoch: 614, loss: 5.595, training accuracy: 0.00%\n",
      "Epoch: 615, loss: 8.917, training accuracy: 0.00%\n",
      "Epoch: 616, loss: 5.023, training accuracy: 0.00%\n",
      "Epoch: 617, loss: 5.720, training accuracy: 10.00%\n",
      "Epoch: 618, loss: 3.738, training accuracy: 10.00%\n",
      "Epoch: 619, loss: 6.974, training accuracy: 0.00%\n",
      "Epoch: 620, loss: 8.200, training accuracy: 0.00%\n",
      "Epoch: 621, loss: 6.171, training accuracy: 0.00%\n",
      "Epoch: 622, loss: 9.400, training accuracy: 0.00%\n",
      "Epoch: 623, loss: 14.978, training accuracy: 0.00%\n",
      "Epoch: 624, loss: 12.620, training accuracy: 0.00%\n",
      "Epoch: 625, loss: 13.468, training accuracy: 0.00%\n",
      "Epoch: 626, loss: 19.296, training accuracy: 0.00%\n",
      "Epoch: 627, loss: 14.885, training accuracy: 0.00%\n",
      "Epoch: 628, loss: 18.844, training accuracy: 0.00%\n",
      "Epoch: 629, loss: 21.506, training accuracy: 0.00%\n",
      "Epoch: 630, loss: 14.539, training accuracy: 0.00%\n",
      "Epoch: 631, loss: 16.742, training accuracy: 0.00%\n",
      "Epoch: 632, loss: 20.688, training accuracy: 0.00%\n",
      "Epoch: 633, loss: 16.212, training accuracy: 0.00%\n",
      "Epoch: 634, loss: 18.057, training accuracy: 0.00%\n",
      "Epoch: 635, loss: 14.350, training accuracy: 0.00%\n",
      "Epoch: 636, loss: 13.928, training accuracy: 0.00%\n",
      "Epoch: 637, loss: 20.814, training accuracy: 0.00%\n",
      "Epoch: 638, loss: 23.949, training accuracy: 0.00%\n",
      "Epoch: 639, loss: 17.969, training accuracy: 0.00%\n",
      "Epoch: 640, loss: 14.089, training accuracy: 20.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 641, loss: 17.920, training accuracy: 0.00%\n",
      "Epoch: 642, loss: 13.505, training accuracy: 0.00%\n",
      "Epoch: 643, loss: 11.085, training accuracy: 20.00%\n",
      "Epoch: 644, loss: 15.494, training accuracy: 0.00%\n",
      "Epoch: 645, loss: 4.537, training accuracy: 10.00%\n",
      "Epoch: 646, loss: 12.140, training accuracy: 0.00%\n",
      "Epoch: 647, loss: 12.016, training accuracy: 0.00%\n",
      "Epoch: 648, loss: 7.097, training accuracy: 0.00%\n",
      "Epoch: 649, loss: 14.233, training accuracy: 0.00%\n",
      "Epoch: 650, loss: 13.061, training accuracy: 0.00%\n",
      "Epoch: 651, loss: 12.657, training accuracy: 0.00%\n",
      "Epoch: 652, loss: 10.754, training accuracy: 0.00%\n",
      "Epoch: 653, loss: 6.595, training accuracy: 0.00%\n",
      "Epoch: 654, loss: 12.402, training accuracy: 0.00%\n",
      "Epoch: 655, loss: 8.480, training accuracy: 0.00%\n",
      "Epoch: 656, loss: 10.877, training accuracy: 10.00%\n",
      "Epoch: 657, loss: 12.363, training accuracy: 0.00%\n",
      "Epoch: 658, loss: 7.803, training accuracy: 0.00%\n",
      "Epoch: 659, loss: 10.427, training accuracy: 0.00%\n",
      "Epoch: 660, loss: 10.427, training accuracy: 0.00%\n",
      "Epoch: 661, loss: 9.691, training accuracy: 0.00%\n",
      "Epoch: 662, loss: 7.719, training accuracy: 0.00%\n",
      "Epoch: 663, loss: 11.434, training accuracy: 0.00%\n",
      "Epoch: 664, loss: 7.520, training accuracy: 0.00%\n",
      "Epoch: 665, loss: 8.974, training accuracy: 0.00%\n",
      "Epoch: 666, loss: 9.052, training accuracy: 0.00%\n",
      "Epoch: 667, loss: 7.320, training accuracy: 10.00%\n",
      "Epoch: 668, loss: 4.452, training accuracy: 10.00%\n",
      "Epoch: 669, loss: 6.500, training accuracy: 0.00%\n",
      "Epoch: 670, loss: 10.798, training accuracy: 0.00%\n",
      "Epoch: 671, loss: 12.493, training accuracy: 10.00%\n",
      "Epoch: 672, loss: 16.892, training accuracy: 0.00%\n",
      "Epoch: 673, loss: 19.794, training accuracy: 0.00%\n",
      "Epoch: 674, loss: 11.256, training accuracy: 0.00%\n",
      "Epoch: 675, loss: 7.741, training accuracy: 10.00%\n",
      "Epoch: 676, loss: 12.493, training accuracy: 0.00%\n",
      "Epoch: 677, loss: 13.611, training accuracy: 10.00%\n",
      "Epoch: 678, loss: 16.200, training accuracy: 0.00%\n",
      "Epoch: 679, loss: 11.450, training accuracy: 0.00%\n",
      "Epoch: 680, loss: 14.680, training accuracy: 10.00%\n",
      "Epoch: 681, loss: 25.907, training accuracy: 0.00%\n",
      "Epoch: 682, loss: 20.067, training accuracy: 0.00%\n",
      "Epoch: 683, loss: 17.019, training accuracy: 10.00%\n",
      "Epoch: 684, loss: 15.767, training accuracy: 0.00%\n",
      "Epoch: 685, loss: 15.459, training accuracy: 10.00%\n",
      "Epoch: 686, loss: 12.594, training accuracy: 0.00%\n",
      "Epoch: 687, loss: 8.880, training accuracy: 0.00%\n",
      "Epoch: 688, loss: 18.022, training accuracy: 0.00%\n",
      "Epoch: 689, loss: 15.228, training accuracy: 0.00%\n",
      "Epoch: 690, loss: 11.777, training accuracy: 10.00%\n",
      "Epoch: 691, loss: 14.617, training accuracy: 0.00%\n",
      "Epoch: 692, loss: 13.157, training accuracy: 0.00%\n",
      "Epoch: 693, loss: 16.047, training accuracy: 0.00%\n",
      "Epoch: 694, loss: 15.069, training accuracy: 0.00%\n",
      "Epoch: 695, loss: 13.315, training accuracy: 0.00%\n",
      "Epoch: 696, loss: 11.358, training accuracy: 0.00%\n",
      "Epoch: 697, loss: 8.654, training accuracy: 0.00%\n",
      "Epoch: 698, loss: 7.838, training accuracy: 0.00%\n",
      "Epoch: 699, loss: 12.396, training accuracy: 0.00%\n",
      "Epoch: 700, loss: 7.638, training accuracy: 20.00%\n",
      "Epoch: 701, loss: 13.572, training accuracy: 10.00%\n",
      "Epoch: 702, loss: 15.039, training accuracy: 0.00%\n",
      "Epoch: 703, loss: 12.820, training accuracy: 0.00%\n",
      "Epoch: 704, loss: 10.391, training accuracy: 0.00%\n",
      "Epoch: 705, loss: 7.513, training accuracy: 0.00%\n",
      "Epoch: 706, loss: 7.450, training accuracy: 0.00%\n",
      "Epoch: 707, loss: 11.707, training accuracy: 0.00%\n",
      "Epoch: 708, loss: 10.547, training accuracy: 0.00%\n",
      "Epoch: 709, loss: 7.014, training accuracy: 0.00%\n",
      "Epoch: 710, loss: 4.892, training accuracy: 0.00%\n",
      "Epoch: 711, loss: 7.904, training accuracy: 0.00%\n",
      "Epoch: 712, loss: 5.682, training accuracy: 10.00%\n",
      "Epoch: 713, loss: 8.710, training accuracy: 0.00%\n",
      "Epoch: 714, loss: 10.926, training accuracy: 0.00%\n",
      "Epoch: 715, loss: 6.695, training accuracy: 10.00%\n",
      "Epoch: 716, loss: 7.605, training accuracy: 0.00%\n",
      "Epoch: 717, loss: 10.512, training accuracy: 0.00%\n",
      "Epoch: 718, loss: 6.285, training accuracy: 0.00%\n",
      "Epoch: 719, loss: 6.860, training accuracy: 0.00%\n",
      "Epoch: 720, loss: 6.336, training accuracy: 0.00%\n",
      "Epoch: 721, loss: 10.837, training accuracy: 0.00%\n",
      "Epoch: 722, loss: 7.805, training accuracy: 0.00%\n",
      "Epoch: 723, loss: 19.686, training accuracy: 0.00%\n",
      "Epoch: 724, loss: 10.215, training accuracy: 0.00%\n",
      "Epoch: 725, loss: 11.132, training accuracy: 0.00%\n",
      "Epoch: 726, loss: 14.475, training accuracy: 0.00%\n",
      "Epoch: 727, loss: 10.561, training accuracy: 10.00%\n",
      "Epoch: 728, loss: 9.312, training accuracy: 0.00%\n",
      "Epoch: 729, loss: 11.239, training accuracy: 0.00%\n",
      "Epoch: 730, loss: 27.142, training accuracy: 0.00%\n",
      "Epoch: 731, loss: 18.698, training accuracy: 0.00%\n",
      "Epoch: 732, loss: 13.571, training accuracy: 0.00%\n",
      "Epoch: 733, loss: 19.319, training accuracy: 0.00%\n",
      "Epoch: 734, loss: 17.334, training accuracy: 0.00%\n",
      "Epoch: 735, loss: 18.649, training accuracy: 10.00%\n",
      "Epoch: 736, loss: 22.920, training accuracy: 0.00%\n",
      "Epoch: 737, loss: 18.547, training accuracy: 0.00%\n",
      "Epoch: 738, loss: 26.883, training accuracy: 0.00%\n",
      "Epoch: 739, loss: 14.610, training accuracy: 0.00%\n",
      "Epoch: 740, loss: 26.470, training accuracy: 0.00%\n",
      "Epoch: 741, loss: 20.611, training accuracy: 10.00%\n",
      "Epoch: 742, loss: 14.607, training accuracy: 10.00%\n",
      "Epoch: 743, loss: 10.413, training accuracy: 0.00%\n",
      "Epoch: 744, loss: 5.831, training accuracy: 0.00%\n",
      "Epoch: 745, loss: 6.020, training accuracy: 0.00%\n",
      "Epoch: 746, loss: 13.377, training accuracy: 0.00%\n",
      "Epoch: 747, loss: 16.732, training accuracy: 0.00%\n",
      "Epoch: 748, loss: 8.361, training accuracy: 10.00%\n",
      "Epoch: 749, loss: 14.571, training accuracy: 0.00%\n",
      "Epoch: 750, loss: 6.641, training accuracy: 0.00%\n",
      "Epoch: 751, loss: 19.155, training accuracy: 10.00%\n",
      "Epoch: 752, loss: 17.551, training accuracy: 0.00%\n",
      "Epoch: 753, loss: 13.983, training accuracy: 0.00%\n",
      "Epoch: 754, loss: 14.233, training accuracy: 0.00%\n",
      "Epoch: 755, loss: 10.230, training accuracy: 0.00%\n",
      "Epoch: 756, loss: 12.288, training accuracy: 0.00%\n",
      "Epoch: 757, loss: 8.879, training accuracy: 0.00%\n",
      "Epoch: 758, loss: 10.620, training accuracy: 0.00%\n",
      "Epoch: 759, loss: 5.918, training accuracy: 0.00%\n",
      "Epoch: 760, loss: 6.647, training accuracy: 0.00%\n",
      "Epoch: 761, loss: 4.901, training accuracy: 0.00%\n",
      "Epoch: 762, loss: 5.271, training accuracy: 0.00%\n",
      "Epoch: 763, loss: 11.598, training accuracy: 0.00%\n",
      "Epoch: 764, loss: 10.622, training accuracy: 0.00%\n",
      "Epoch: 765, loss: 13.279, training accuracy: 0.00%\n",
      "Epoch: 766, loss: 10.314, training accuracy: 0.00%\n",
      "Epoch: 767, loss: 4.545, training accuracy: 0.00%\n",
      "Epoch: 768, loss: 7.372, training accuracy: 0.00%\n",
      "Epoch: 769, loss: 5.184, training accuracy: 0.00%\n",
      "Epoch: 770, loss: 6.619, training accuracy: 0.00%\n",
      "Epoch: 771, loss: 10.523, training accuracy: 0.00%\n",
      "Epoch: 772, loss: 9.577, training accuracy: 0.00%\n",
      "Epoch: 773, loss: 10.988, training accuracy: 0.00%\n",
      "Epoch: 774, loss: 9.001, training accuracy: 0.00%\n",
      "Epoch: 775, loss: 11.633, training accuracy: 0.00%\n",
      "Epoch: 776, loss: 15.320, training accuracy: 0.00%\n",
      "Epoch: 777, loss: 13.546, training accuracy: 0.00%\n",
      "Epoch: 778, loss: 20.947, training accuracy: 0.00%\n",
      "Epoch: 779, loss: 15.495, training accuracy: 10.00%\n",
      "Epoch: 780, loss: 16.878, training accuracy: 0.00%\n",
      "Epoch: 781, loss: 16.929, training accuracy: 0.00%\n",
      "Epoch: 782, loss: 16.208, training accuracy: 0.00%\n",
      "Epoch: 783, loss: 16.440, training accuracy: 0.00%\n",
      "Epoch: 784, loss: 19.316, training accuracy: 0.00%\n",
      "Epoch: 785, loss: 16.476, training accuracy: 0.00%\n",
      "Epoch: 786, loss: 13.791, training accuracy: 0.00%\n",
      "Epoch: 787, loss: 15.037, training accuracy: 0.00%\n",
      "Epoch: 788, loss: 22.393, training accuracy: 0.00%\n",
      "Epoch: 789, loss: 21.964, training accuracy: 0.00%\n",
      "Epoch: 790, loss: 10.041, training accuracy: 0.00%\n",
      "Epoch: 791, loss: 12.602, training accuracy: 10.00%\n",
      "Epoch: 792, loss: 19.775, training accuracy: 0.00%\n",
      "Epoch: 793, loss: 11.467, training accuracy: 0.00%\n",
      "Epoch: 794, loss: 12.172, training accuracy: 10.00%\n",
      "Epoch: 795, loss: 10.855, training accuracy: 10.00%\n",
      "Epoch: 796, loss: 13.925, training accuracy: 0.00%\n",
      "Epoch: 797, loss: 11.445, training accuracy: 0.00%\n",
      "Epoch: 798, loss: 9.683, training accuracy: 0.00%\n",
      "Epoch: 799, loss: 9.988, training accuracy: 10.00%\n",
      "Epoch: 800, loss: 14.405, training accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# run the training\n",
    "epochs = training_epochs\n",
    "results = pd.DataFrame(columns=['loss','accuracy'])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(training_init_op)\n",
    "    train_summary_writer = tf.summary.FileWriter('./tmp/train',sess.graph)\n",
    "    test_summary_writer = tf.summary.FileWriter('./tmp/test')\n",
    "    for i in range(epochs):\n",
    "        log,lab,pr = sess.run([logits,labels,pred])\n",
    "        l, _, acc,merged_summary = sess.run([loss, optimizer, accuracy,merged_summary_operation])\n",
    "        \n",
    "        print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i+1, l, acc * 100))\n",
    "        '''\n",
    "        # now setup the validation run\n",
    "        test_iters = 1\n",
    "        # re-initialize the iterator, but this time with validation data\n",
    "        sess.run(test_init_op)\n",
    "        test_l, test_acc = sess.run([loss, accuracy])\n",
    "        print(\"Epoch: {}, test loss: {:.3f}, test accuracy: {:.2f}%\".format(i+1, test_l, test_acc * 100))\n",
    "        '''\n",
    "        results.loc[i] = [l,acc]\n",
    "        train_summary_writer.add_summary(merged_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir='./tmp/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
