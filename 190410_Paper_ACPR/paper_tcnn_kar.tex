% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage{amsmath}

\begin{document}
%
\title{Deep triplet network adopting the kernel and the range space learning for Wi-Fi signature verification}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{YoungWoong KWON\inst{1} \and
JooYoung KIM\inst{1} \and
Kar-Ann TOH\inst{1}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Yonsei University \and
\email{lncs@springer.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
15--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%
\section{Introduction}

\subsection{Motivation}
i. Pros of In-Air WIFI CSI signature system
1) Cheap: Use commercial device
2) Easy: No additional devices is needed
3) Secure: Hard to forgery
ii. Cons of In-Air WIFI CSI signature system
1) Setting direction problem
a) Different direction -> Different feature is needed
b) Hard to set exactly same direction as authentication before
Size of signature can varies

\subsection{Contribution}
- Overcome cons of WIFI signature system
- Robust to signal direction, size

\section{Related Works}

\subsection{WIFI CSI}
- An In-Air Signature Verification System Using Wi-Fi Signals 

% Meaning of CSI
%[From Halperin paper]
CSI captures signal strength and phase information for OFDM subcarriers and between each pair of transmit-receive antennas.
It runs on a commodity 802.11n NIC, and records Channel State Information (CSI) based on the 802.11 standard.
The CSI contains information about the channel between sender and receiver at the level of individual data subcarriers, for each pair of transmit and receive antennas.
%[From Halperin End]

% Structure of CSI
%[From HC's ELM paper]
In a frequency domain, the CSI of sub-carrier $\mathbf{c}$ between transmitter(Tx) and receiver(Rx) can be modeled as 
$\mathnormal{R}_{c} = \mathbf{H}_{c}\mathnormal{T}_{c} +\mathnormal{N}$ where the $\mathnormal{R}_{c}$ and $\mathnormal{T}_{c}$  denote the received and the transmitted signal vector of dimension $\mathnormal{r}$ and $\mathnormal{t}$, respectively. The $\mathnormal{N}$ is the additive channel noise and $\mathbf{H}_{c}$ is the $\mathnormal{r}\times\mathnormal{t}$ channel matrix. The CSI of sub-carrier $\mathnormal{c}$ can be modeled as follows:
\begin{equation}
    \mathnormal{h}_{c} = \mid\mathnormal{h}_{c}\mid\mathnormal{e}^{\angle\theta},
\end{equation}
where $\mid\mathnormal{h}_{c}\mid$ and $\theta$ represent the amplitude and the phase of the sub-carrier, respectively.
%[From HC's ELM end]

\subsection{ConvNet with triplet loss}

% description of siamese net
In \cite{bromley1994signature}, LeCun et al. Introduced Siamese nets as parts of their handwritten signature verification system. 
A siamese neural network consists of twin networks which accept distinct inputs but are joined by an energy function at the top.\cite{koch2015siamese}
% differences in feature extractor
They proposed a feature extractor based on Time Delay Neural Networks\cite{lang1990time} and feature matcher based on cosine distance of feature vector. Apart from this work based on ConvNet as feature extractor and L1 distance as feature matcher.
In this paper, we propose a for signature verification system that is applicable for WIFI CSI signal, which is more complex and larger than handwritten images. 
% advantages of using Convnet filter
Based on Convnets as feature extractor, we are able to make feature vector from CSI signal reflecting local connectivity between signals at a near frequency range and closer measurement time. 
The proposed method achieved better performance because it is applicable to all points of CSI signals. This is due to the weight sharing the property of the Convnet filter.

\section{Proposed System}

In this section, we propose a direction-free identify verification system based on the Wi-Fi based in-air handwritten signature (will be called Wi-Fi signature signals hereafter). An overview of the proposed system utilizing the ConvNet with triplet loss is shown in Fig.1.
Essentially, the Wi-Fi signature signals are preprocessed to create the input data for our network. Subsequently, the ConvNet structure is trained to minimize distance from anchor to positive input while maximize distance from anchor to negative inputs. The following subsections detail the data preprocessing and the proposed method.

% Figure 1
\begin{figure}
    \includegraphics[width=\textwidth]{fig1_network_structure.pdf}
    \caption{Structure of the network} \label{fig1}
\end{figure}

\subsection{Data Preprocessing}

Since every Wi-Fi signature signal has different data size, we firstly adopted the gradient operation with respect to the time instance to measure the short time energy. Data points with the highest short-time energy within the time period are then manually selected as the starting and the ending points of the in-air signature action. Subsequently, the Fast Fourier Transform based re-sampling method \cite{moon2017air} is implemented to unify the length of the signals. As a result, three-dimensional Wi-Fi signature signals with unified data size are obtained as the input of the ConvNet structure in the network.

% Methods 
% Section: Feature Extraction
\subsection{ConvNets with triplet loss}

To design the proposed networks, we firstly need to select the feature extracting networks which convert the input data into a vector. In this work, we utilize the ConvNet structure \cite{lecun1998gradient} as a feature extractor since the three-dimensional data format of our preprocessed input signal can be regarded as an image data format with multiple channels. 

Our ConvNet structure (See Fig~\ref{fig1} (b)) for the network consists of $i$ convolutional layers $\mathbf{C}_{i}$ and one fully-connected layer $\mathbf{F}$. The number of convolutional filters to be trained in each layer is empirically chosen as $\{64, 128, ...,  2^{6+i}\}$, with fixed filter size of $3\times3$ and stride of 1. The RectiÔ¨Åed Linear (ReLU) function as an activation function and the max-pooling layers are applied between each convolutional layers. The features from the last convolutional layer are directly flattened into a single vector without activation function and the Max-pooling layer followed by the fully-connected layer.
%Since the networks utilize three ConvNet structures which ties the weights each other, noting here that three structures described in Fig~\ref{fig1} (b) are actually the same model.

In our proposed networks, we utilize the $L_2$ distance to calculate the triplet loss. For the $i_{th}$ signal as anchor input $\mathbf{I}_{i,anc}$, positive input $\mathbf{I}_{i,pos}$ is belong to the same class for anchor input while negative input $\mathbf{I}_{i,neg}$ is from another class.
Let $\mathbf{x}_{i,anc}\in{\mathrm{R}}^{d\times1}$, $\mathbf{x}_{i,pos}\in{\mathrm{R}}^{d\times1}$ and $\mathbf{x}_{i,neg}\in{\mathrm{R}}^{d\times1}$ be three feature vectors extracted from the ConvNet structure. Then the loss for the triplet inputs can be calculated as follows:
\begin{equation}
loss = \sum_i^N { \left[ {\left\| {{\mathbf{x}_{i,anc}} - {\mathbf{x}_{i,pos}}} \right\|_2^2} -
{\left\| {{\mathbf{x}_{i,anc}} - {\mathbf{x}_{i,neg}}} \right\|_2^2}  + \alpha \right]_+},
\end{equation} 
where $\alpha$ is the margin to make distance between positive and negative pairs, N is size of the mini-batch. 

% KAR learning
\subsection{Triplet selection by the kernel and the range space learning}

% importance of selecting hard pos/neg
in \cite{schroff2015facenet}, when training triplet loss networks, it is important to take hard positive and hard negative sample for faster convergence of the loss function.

% methods of KAR learning
since we don't know which is the hard sample before training entire network, we make multilayer feedforward neural network instead to select the hard positive and the hard negative.
we adopted gradient-free learning to train this network. by the kernel and the range (KAR) space projection learning \cite{toh2018learning,toh2018gradient}.

% training KARnet
first, let the training dataset $\mathbf{X}\in{\mathrm{R}}^{m \times (n+1)}$ and one-hot encoded target $\mathbf{Y}\in{\mathrm{R}}^{m \times n}$. 

set multilayer neural network structure 
\begin{equation}
\mathbf{Y} = \sigma\left(\left[\mathbf{1},\sigma\left(\dots\left[\mathbf{1},\sigma\left(\left[\mathbf{1},\sigma\left(\mathbf{X}\mathbf{W}_{1}\right)\right]\mathbf{W}_{2}\right)\right]\dots\mathbf{W}_{(i-1)}\right)\right]\mathbf{W}_{i}\right),
\end{equation}

where $\mathbf{W}_{1}\in{\mathrm{R}}^{(n+1) \times h_{1}}$,$\mathbf{W}_{2}\in{\mathrm{R}}^{(h_{1}+1) \times h_{2}}$,$\dots,\mathbf{W}_{i}\in{\mathrm{R}}^{(h_{(i-1)}+1) \times n}$,
$\mathbf{1}=\left[1,\dots,1\right]^{T}\in{\mathrm{R}}^{m \times 1}$
and $\sigma(.)$ is activation function.

the weight matrix $\mathbf{W}_{1}\dots\mathbf{W}_{i}$ can be separated into weights and bias term,
$\mathbf{W}_{2}\dots\mathbf{W}_{i}$ = 
$\begin{bmatrix}
\mathbf{w}_{2}^{T}\\\mathnormal{W}_{2}
\end{bmatrix}
\dots
\begin{bmatrix}
\mathbf{w}_{i}^{T}\\\mathnormal{W}_{i}
\end{bmatrix}$

after assign random weights to $\mathbf{W}_{1}\dots\mathbf{W}_{i}$ , we can get $\mathbf{W}_{1}$. the equation can be solved as follows:

\begin{equation}
\left[\sigma^{-1}\left(\mathbf{Y}\right)-\mathbf{1}\cdot\mathbf{w}_{i}^{T}\right]\mathnormal{W}_{i}^\dagger = 
\sigma\left(\dots\left[\mathbf{1},\sigma\left(\left[\mathbf{1},\sigma\left(\mathbf{X}\mathbf{W}_{1}\right)\right]\mathbf{W}_{2}\right)\right]\dots\mathbf{W}_{(i-1)}\right)
\end{equation}

%\begin{equation}
\begin{multline*}
\Rightarrow
\biggl[\sigma^{-1}\biggl(
\dots
\biggl[\sigma^{-1}\biggl(
    \left[\sigma^{-1}\left(\mathbf{Y}\right)-\mathbf{1}\cdot\mathbf{w}_{i}^{T}\right]\mathnormal{W}_{i}^\dagger
\biggr)\\
- \mathbf{1}\cdot\mathbf{w}_{(i-1)}^{T}\biggr]\mathnormal{W}_{(i-1)}^{\dagger}
\dots\biggr) 
- \mathbf{1}\cdot\mathbf{w}_{2}^{T}\biggr]\mathnormal{W}_{2}^{\dagger} 
= \sigma\left(\mathbf{X}\mathbf{W}_{1}\right)
\end{multline*}
%\end{equation}

\begin{multline*}
\Rightarrow
\mathbf{X}^{\dagger}\sigma^{-1}\biggl(
\biggl[\sigma^{-1}\biggl(
\dots
\biggl[\sigma^{-1}\biggl(
    \left[\sigma^{-1}\left(\mathbf{Y}\right)-\mathbf{1}\cdot\mathbf{w}_{i}^{T}\right]\mathnormal{W}_{i}^\dagger
\biggr)\\
- \mathbf{1}\cdot\mathbf{w}_{(i-1)}^{T}\biggr]\mathnormal{W}_{(i-1)}^{\dagger}
\dots\biggr)
- \mathbf{1}\cdot\mathbf{w}_{2}^{T}\biggr]\mathnormal{W}_{2}^{\dagger}
= \mathbf{W}_{1}
\end{multline*}

after getting $\mathbf{W}_{1}$, $\mathbf{W}_{2}$ can also be optimized

\begin{multline*}
\Rightarrow
\left(\sigma\left(\mathbf{X}\mathbf{W}_{1}\right)\right)^{\dagger}
\biggl(\dots
\biggl[\sigma^{-1}\biggl(
    \left[\sigma^{-1}\left(\mathbf{Y}\right)-\mathbf{1}\cdot\mathbf{w}_{i}^{T}\right]\mathnormal{W}_{i}^\dagger
\biggr)
- \mathbf{1}\cdot\mathbf{w}_{(i-1)}^{T}\biggr]\mathnormal{W}_{(i-1)}^{\dagger}
\dots\biggr)\\ 
= \mathbf{W}_{2}
\end{multline*}

Repeat this process recursively until all weight maxtrix values are obtained.
finally, $\mathbf{W}_{i}$ can be obtained as follows:

\begin{equation}
    \mathbf{W}_{i} = \left[\mathbf{1},\sigma\left(\dots\left[\mathbf{1},\sigma\left(\left[\mathbf{1},\sigma\left(\mathbf{X}\mathbf{W}_{1}\right)\right]\mathbf{W}_{2}\right)\right]\dots\mathbf{W}_{(i-1)}\right)\right]^{\dagger}\sigma^{-1}\left(\mathbf{Y}\right),
\end{equation}

\section{Experiments}
% Dataset
\subsubsection{Dataset}
 To evaluate validation performance of proposed system, Wi-Fi CSI signature dataset from \cite{moon2017air} was used.
 % description
 we utilized 2000 Wi-Fi CSI signature signals (4 directions $\times$ 50 identities $\times$ 10 samples) which is dimension of (500 packets $\times$ 30 subcarriers $\times$ 6 antennas). 
 
% network shape

% network parameters

% Network Learning
\subsubsection{Loss and Backpropagation}
 We impose a regularized cross-entropy objective on our binary classifier.
This objective is combined with standard backpropagation algorithm, where the gradient is additive across the twin networks due to the tied weights.
 We initialized all network weights in the convolutional layers from a normal distribution with zero-mean and a standard deviation of 10‚àí2. Biases were also initialized from a normal distribution, but with mean 0.5 and standard deviation 10‚àí2.

\section{Conclusion}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
%\bibliographystyle{splncs04}
%\bibliography{mybibliography}
%


%\begin{thebibliography}{8}

\bibliographystyle{splncs04}
\bibliography{bib_acpr}

%\end{thebibliography}

\end{document}