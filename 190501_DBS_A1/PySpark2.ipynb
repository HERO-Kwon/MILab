{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Py. SP: Import Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Py. SP: Import Library\n",
    "# Import Library\n",
    "# Python\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd # for data manipulation\n",
    "import matplotlib.pyplot as plt # for graph\n",
    "\n",
    "# SPARK\n",
    "import pyspark\n",
    "import findspark # to find location where spark installed\n",
    "findspark.init()\n",
    "from pyspark.context import SparkConf, SparkContext\n",
    "from pyspark.sql.session import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check spark version\n",
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Py.SP: Make Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify master\n",
    "conf = (SparkConf()\n",
    "        .setMaster(\"spark://192.168.10.3:7077\"))\n",
    "sc = SparkContext(conf = conf)\n",
    "# make session\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Set Target Animal\n",
    "target_animal = 'alligator'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Py.SP: Make LBP Features From Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Py.SP: Make LBP Features From Images\n",
    "\n",
    "# SPARK: read image files in directory and make it to dataframe\n",
    "#img_dir = \"D:\\\\Data\\\\AnimalsOnTheWeb\\\\\" + target_animal\n",
    "#imgs = spark.read.format(\"image\").load(img_dir)\n",
    "#imgs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SPARK: Read Feature CSV file and make DataFrame\n",
    "import pyspark.sql.types as typ\n",
    "#res_lbp = spark.read.csv('Res_LBP_color.csv',header=True)\n",
    "labels =[\n",
    "    ('ind',typ.IntegerType()), # index\n",
    "    ('Animal',typ.StringType()), # Class of animals\n",
    "    ('File',typ.StringType()), # filename\n",
    "    ('ID',typ.StringType()), # picture ID\n",
    "    ('LBP0',typ.FloatType()), # LBP features\n",
    "    ('LBP1',typ.FloatType()),\n",
    "    ('LBP2',typ.FloatType()),\n",
    "    ('LBP3',typ.FloatType()),\n",
    "    ('LBP4',typ.FloatType()),\n",
    "    ('LBP5',typ.FloatType()),\n",
    "    ('LBP6',typ.FloatType()),\n",
    "    ('LBP7',typ.FloatType()),\n",
    "    ('LBP8',typ.FloatType()),\n",
    "    ('LBP9',typ.FloatType()),\n",
    "    ('CR0',typ.FloatType()),\n",
    "    ('CR1',typ.FloatType()),\n",
    "    ('CR2',typ.FloatType()),\n",
    "    ('CR3',typ.FloatType()),\n",
    "    ('CG0',typ.FloatType()),\n",
    "    ('CG1',typ.FloatType()),\n",
    "    ('CG2',typ.FloatType()),\n",
    "    ('CG3',typ.FloatType()),\n",
    "    ('CB0',typ.FloatType()),\n",
    "    ('CB1',typ.FloatType()),\n",
    "    ('CB2',typ.FloatType()),\n",
    "    ('CB3',typ.FloatType())\n",
    "]\n",
    "# Define Schema\n",
    "schema = typ.StructType([\n",
    "    typ.StructField(e[0],e[1],False) for e in labels\n",
    "])\n",
    "\n",
    "# CSV read\n",
    "res_lbp = spark.read.csv('Res_LBP_color.csv',header=True,schema=schema)\n",
    "# Select Target Animal\n",
    "target_lbp = res_lbp.where(res_lbp.Animal.isin(target_animal))\n",
    "target_lbp.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_lbp.show() # show 20 row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Py.SP: Join Ground Truth and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Py.SP: Make Ground Truth\n",
    "\n",
    "# Python: Read .mat file\n",
    "import scipy.io as sio # Library for .mat files\n",
    "import re # Library for Regular Expression\n",
    "file_path = 'D:\\\\Data\\\\AnimalsOnTheWeb\\\\' + target_animal + '\\\\'\n",
    "file = 'animaldata_'+ target_animal + '.mat'\n",
    "# Read from .mat files\n",
    "data_read = sio.loadmat(os.path.join(file_path,file))\n",
    "\n",
    "# truth table (1 or 0)\n",
    "truth_tbl = list(data_read['gt'][0]) \n",
    "\n",
    "# get picture ID and save it to 'name' column\n",
    "truth_nameread = list(data_read['imgnames'][0])\n",
    "truth_name = [t[0] for t in truth_nameread]\n",
    "truth_lists = pd.DataFrame({'name': truth_name,'truth': truth_tbl})\n",
    "truth_lists['name'] = truth_lists['name'].astype('str')\n",
    "re_picid = re.compile('pic\\d+')\n",
    "truth_lists['ID'] = [re_picid.findall(r)[0] for r in truth_lists['name']]\n",
    "truth_lists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK: convert pandas DF to Spark DF\n",
    "df_truth = spark.createDataFrame(truth_lists)\n",
    "df_truth.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Truth column to integer\n",
    "df_truth = df_truth.withColumn('truth_int',df_truth['truth'].cast(typ.IntegerType()))\n",
    "df_truth.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show 5 row\n",
    "df_truth.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. SP: join  features and Grd Truth dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5. SP: join features and Grd Truth dataframe\n",
    "df_ml = df_truth.join(target_lbp,on='ID')\n",
    "df_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns from dataframe\n",
    "df_ml1 = df_ml.select([c for c in df_ml.columns if c in ['truth_int','LBP0','LBP1','LBP2','LBP3','LBP4','LBP5','LBP6','LBP7','LBP8','LBP9',\\\n",
    "                                                        'CR0','CR1','CR2','CR3','CB0','CB1','CB2','CB3','CG0','CG1','CG2','CG3']])\n",
    "df_ml1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SP: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Feature Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Feature column\n",
    "import pyspark.ml.feature as ft\n",
    "labels_feat =[\n",
    "    ('LBP0',typ.FloatType()),\n",
    "    ('LBP1',typ.FloatType()),\n",
    "    ('LBP2',typ.FloatType()),\n",
    "    ('LBP3',typ.FloatType()),\n",
    "    ('LBP4',typ.FloatType()),\n",
    "    ('LBP5',typ.FloatType()),\n",
    "    ('LBP6',typ.FloatType()),\n",
    "    ('LBP7',typ.FloatType()),\n",
    "    ('LBP8',typ.FloatType()),\n",
    "    ('LBP9',typ.FloatType()),\n",
    "    ('CR0',typ.FloatType()),\n",
    "    ('CR1',typ.FloatType()),\n",
    "    ('CR2',typ.FloatType()),\n",
    "    ('CR3',typ.FloatType()),\n",
    "    ('CG0',typ.FloatType()),\n",
    "    ('CG1',typ.FloatType()),\n",
    "    ('CG2',typ.FloatType()),\n",
    "    ('CG3',typ.FloatType()),\n",
    "    ('CB0',typ.FloatType()),\n",
    "    ('CB1',typ.FloatType()),\n",
    "    ('CB2',typ.FloatType()),\n",
    "    ('CB3',typ.FloatType()),\n",
    "]\n",
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[col[0] for col in labels_feat[0:]],outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Make Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model\n",
    "import pyspark.ml.classification as cl\n",
    "logistic = cl.LogisticRegression(maxIter=10,regParam=0.01,labelCol='truth_int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[featuresCreator,logistic])\n",
    "\n",
    "# Separate training and test data\n",
    "lbp_train, lbp_test = df_ml1.randomSplit([0.7,0.3],seed=100)\n",
    "# Train model\n",
    "model = pipeline.fit(lbp_train)\n",
    "# Test\n",
    "test_model = model.transform(lbp_test) # get results on test dataset\n",
    "#test_model.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. SP: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability',labelCol='truth_int')\n",
    "print('Area Under ROC: ' + str(evaluator.evaluate(test_model, {evaluator.metricName:'areaUnderROC'})))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
